{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6gtEAmhXsu3Xxcyw6wTeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvtien96/CORING/blob/main/notebooks/Random_vs_Norm_based_vs_Similarity_based_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Introduction**"
      ],
      "metadata": {
        "id": "lVeZS2NhLJ3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is the second episode in the serie of the project filter pruning.\n",
        "\n",
        "## Purpose:\n",
        "Accelerate the project.\n",
        "\n",
        "## Key Components:\n",
        "1. **Load baseline model**: Load a pretrained model instead of training the baseline from scratch, to save resource. *You don't need to worry about these details initially*.\n",
        "\n",
        "2. **Evaluate a model**: Evaluate a model through 3 criteria: accuracy, number of parameter and MACs.\n",
        "\n",
        "2. **Add functions**: Add 3 functions for random, norm-based and distance-based pruning.\n",
        "\n",
        "3. **Fine-tuning**: Fine-tunes the pruned model to assess its performance in terms of accuracy and efficiency compared to the original model.\n",
        "\n",
        "4. **Analysis & Conclusion**: Analyze the results, highlights insights gained from the experiment, and provides suggestions for further exploration or improvement.\n",
        "\n",
        "## Prerequisite:\n",
        "- [First notebook](https://github.com/pvtien96/CORING/blob/main/notebooks/Similarity_based_Filter_Pruning.ipynb)\n",
        "\n",
        "Let's dive in and explore the exciting world of filter pruning and deep learning efficiency optimization!\n"
      ],
      "metadata": {
        "id": "2vrswGamLOg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 2: Setup**"
      ],
      "metadata": {
        "id": "5YODvlrtPFUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment"
      ],
      "metadata": {
        "id": "y3TyDMJCPLN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_9R7-eAPUUy",
        "outputId": "50ddf37d-19b3-4469-ef8b-572c51b9f1fa"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define VGG-16-BN model"
      ],
      "metadata": {
        "id": "bvA-fbFDPq9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "defaultcfg = [\n",
        "    64,\n",
        "    64,\n",
        "    \"M\",\n",
        "    128,\n",
        "    128,\n",
        "    \"M\",\n",
        "    256,\n",
        "    256,\n",
        "    256,\n",
        "    \"M\",\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "    \"M\",\n",
        "    512,\n",
        "    512,\n",
        "    512,\n",
        "]\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, compress_rate=[0.0] * 13, cfg=None, num_classes=10):\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        if cfg is None:\n",
        "            cfg = defaultcfg\n",
        "\n",
        "        self.compress_rate = compress_rate[:]\n",
        "\n",
        "        self.features = self._make_layers(cfg)\n",
        "        last_conv_out_channels = self.features[-3].out_channels\n",
        "        self.classifier = nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\"linear1\", nn.Linear(last_conv_out_channels, cfg[-1])),\n",
        "                    (\"norm1\", nn.BatchNorm1d(cfg[-1])),\n",
        "                    (\"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\"linear2\", nn.Linear(cfg[-1], num_classes)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = nn.Sequential()\n",
        "        in_channels = 3\n",
        "        cnt = 0\n",
        "\n",
        "        for i, x in enumerate(cfg):\n",
        "            if x == \"M\":\n",
        "                layers.add_module(\"pool%d\" % i, nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            else:\n",
        "                x = int(x * (1 - self.compress_rate[cnt]))\n",
        "                cnt += 1\n",
        "                conv2d = nn.Conv2d(in_channels, x, kernel_size=3, padding=1)\n",
        "                layers.add_module(\"conv%d\" % i, conv2d)\n",
        "                layers.add_module(\"norm%d\" % i, nn.BatchNorm2d(x))\n",
        "                layers.add_module(\"relu%d\" % i, nn.ReLU(inplace=True))\n",
        "                in_channels = x\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def vgg_16_bn(compress_rate=[0.0] * 13):\n",
        "    return VGG(compress_rate=compress_rate)"
      ],
      "metadata": {
        "id": "PL0WKYG_PYg7"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "BsmPZQRgQb2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_cpr(compress_rate):\n",
        "    cprate_str = compress_rate\n",
        "    cprate_str_list = cprate_str.split(\"+\")\n",
        "    pat_cprate = re.compile(r\"\\d+\\.\\d*\")\n",
        "    pat_num = re.compile(r\"\\*\\d+\")\n",
        "    cprate = []\n",
        "    for x in cprate_str_list:\n",
        "        num = 1\n",
        "        find_num = re.findall(pat_num, x)\n",
        "        if find_num:\n",
        "            assert len(find_num) == 1\n",
        "            num = int(find_num[0].replace(\"*\", \"\"))\n",
        "        find_cprate = re.findall(pat_cprate, x)\n",
        "        assert len(find_cprate) == 1\n",
        "        cprate += [float(find_cprate[0])] * num\n",
        "\n",
        "    return cprate"
      ],
      "metadata": {
        "id": "stzE4MnmQE1b"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import time, datetime\n",
        "import logging\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "\n",
        "\n",
        "'''record configurations'''\n",
        "class record_config():\n",
        "    def __init__(self, args):\n",
        "        now = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "        today = datetime.date.today()\n",
        "\n",
        "        self.args = args\n",
        "        self.job_dir = Path(args.job_dir)\n",
        "\n",
        "        def _make_dir(path):\n",
        "            if not os.path.exists(path):\n",
        "                os.makedirs(path)\n",
        "\n",
        "        _make_dir(self.job_dir)\n",
        "\n",
        "        config_dir = self.job_dir / 'config.txt'\n",
        "        #if not os.path.exists(config_dir):\n",
        "        if args.resume:\n",
        "            with open(config_dir, 'a') as f:\n",
        "                f.write(now + '\\n\\n')\n",
        "                for arg in vars(args):\n",
        "                    f.write('{}: {}\\n'.format(arg, getattr(args, arg)))\n",
        "                f.write('\\n')\n",
        "        else:\n",
        "            with open(config_dir, 'w') as f:\n",
        "                f.write(now + '\\n\\n')\n",
        "                for arg in vars(args):\n",
        "                    f.write('{}: {}\\n'.format(arg, getattr(args, arg)))\n",
        "                f.write('\\n')\n",
        "\n",
        "\n",
        "def get_logger(file_path):\n",
        "\n",
        "    logger = logging.getLogger('gal')\n",
        "    log_format = '%(asctime)s | %(message)s'\n",
        "    formatter = logging.Formatter(log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "    file_handler = logging.FileHandler(file_path)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    stream_handler.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(stream_handler)\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    return logger\n",
        "\n",
        "#label smooth\n",
        "class CrossEntropyLabelSmooth(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes, epsilon):\n",
        "    super(CrossEntropyLabelSmooth, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.epsilon = epsilon\n",
        "    self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "    log_probs = self.logsoftmax(inputs)\n",
        "    targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)\n",
        "    targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
        "    loss = (-targets * log_probs).mean(0).sum()\n",
        "    return loss\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print(' '.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, save):\n",
        "    if not os.path.exists(save):\n",
        "        os.makedirs(save)\n",
        "    filename = os.path.join(save, 'checkpoint.pth.tar')\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        best_filename = os.path.join(save, 'model_best.pth.tar')\n",
        "        shutil.copyfile(filename, best_filename)\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "\n",
        "def progress_bar(current, total, msg=None):\n",
        "    _, term_width = os.popen('stty size', 'r').read().split()\n",
        "    term_width = int(term_width)\n",
        "\n",
        "    TOTAL_BAR_LENGTH = 65.\n",
        "    last_time = time.time()\n",
        "    begin_time = last_time\n",
        "\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "metadata": {
        "id": "7MuvmxS1Sa-r"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, train_loader, model, criterion, optimizer, scheduler):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    cur_lr = optimizer.param_groups[0]['lr']\n",
        "    print('learning_rate: ' + str(cur_lr))\n",
        "\n",
        "    num_iter = len(train_loader)\n",
        "    print_freq = num_iter // 10\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        images = images.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n",
        "        n = images.size(0)\n",
        "        losses.update(loss.item(), n)  # accumulated loss\n",
        "        top1.update(prec1.item(), n)\n",
        "        top5.update(prec5.item(), n)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print(\n",
        "                'Epoch[{0}]({1}/{2}): '\n",
        "                'Loss {loss.avg:.4f} '\n",
        "                'Prec@1(1,5) {top1.avg:.2f}, {top5.avg:.2f} '\n",
        "                'Lr {cur_lr:.4f}'.format(\n",
        "                    epoch, i, num_iter, loss=losses,\n",
        "                    top1=top1, top5=top5, cur_lr=cur_lr))\n",
        "    scheduler.step()\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            pred1, pred5 = accuracy(logits, target, topk=(1, 5))\n",
        "            n = images.size(0)\n",
        "            losses.update(loss.item(), n)\n",
        "            top1.update(pred1[0], n)\n",
        "            top5.update(pred5[0], n)\n",
        "\n",
        "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "                    .format(top1=top1, top5=top5))\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg"
      ],
      "metadata": {
        "id": "QpORMFk9SFsC"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def load_data(batch_size=128):\n",
        "\n",
        "    # load training data\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    trainset = torchvision.datasets.CIFAR10(root=\"./\", train=True, download=True,\n",
        "                                            transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    testset = torchvision.datasets.CIFAR10(root=\"./\", train=False, download=True, transform=transform_test)\n",
        "    val_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "wOEgSgHfRYBb"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "epochs = 100\n",
        "lr_warmup_epochs=5\n",
        "lr=0.01\n",
        "momentum=0.9\n",
        "weight_decay=5e-4\n",
        "lr_warmup_decay=0.01"
      ],
      "metadata": {
        "id": "85hVYLlKVtUb"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune(model, train_loader, val_loader, epochs, criterion):\n",
        "    optimizer = torch.optim.SGD(model.parameters(\n",
        "    ), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    main_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=epochs-lr_warmup_epochs)\n",
        "    warmup_lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer, start_factor=lr_warmup_decay, total_iters=lr_warmup_epochs)\n",
        "    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "        optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[lr_warmup_epochs])\n",
        "\n",
        "    _, best_top1_acc, _ = validate(val_loader, model, criterion)\n",
        "    best_model_state = copy.deepcopy(model.state_dict())\n",
        "    epoch = 0\n",
        "    while epoch < epochs:\n",
        "        train(epoch, train_loader, model, criterion,\n",
        "              optimizer, scheduler)\n",
        "        _, valid_top1_acc, _ = validate(val_loader, model, criterion)\n",
        "\n",
        "        if valid_top1_acc > best_top1_acc:\n",
        "            best_top1_acc = valid_top1_acc\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "        epoch += 1\n",
        "        print('=>Best accuracy {:.3f}'.format(best_top1_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "tqqne4ZjX5ab"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 3: Load the pretrained baseline model**"
      ],
      "metadata": {
        "id": "L9DHHosHQ0KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/pvtien96/CORING/releases/download/v0.1.0/vgg_16_bn.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuzsnEEyV7o1",
        "outputId": "c0712418-ff70-40da-ae6f-ffb796306ee7"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-04 15:33:18--  https://github.com/pvtien96/CORING/releases/download/v0.1.0/vgg_16_bn.pt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/572465934/6bb9aca3-1335-40ce-8a25-df08be78e4eb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240404%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240404T153318Z&X-Amz-Expires=300&X-Amz-Signature=f6bea0f40e9058ac9104ac0a5b8a407d24ba226d1991c385e7b456bed9d44270&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=572465934&response-content-disposition=attachment%3B%20filename%3Dvgg_16_bn.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-04-04 15:33:18--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/572465934/6bb9aca3-1335-40ce-8a25-df08be78e4eb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240404%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240404T153318Z&X-Amz-Expires=300&X-Amz-Signature=f6bea0f40e9058ac9104ac0a5b8a407d24ba226d1991c385e7b456bed9d44270&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=572465934&response-content-disposition=attachment%3B%20filename%3Dvgg_16_bn.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 119998475 (114M) [application/octet-stream]\n",
            "Saving to: ‘vgg_16_bn.pt.1’\n",
            "\n",
            "vgg_16_bn.pt.1      100%[===================>] 114.44M   260MB/s    in 0.4s    \n",
            "\n",
            "2024-04-04 15:33:19 (260 MB/s) - ‘vgg_16_bn.pt.1’ saved [119998475/119998475]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import copy\n",
        "\n",
        "\n",
        " # initialize model\n",
        "model_ori = vgg_16_bn(compress_rate=[0.0]*13).cuda()\n",
        "print(model_ori)\n",
        "\n",
        "# load training data\n",
        "train_loader, val_loader = load_data()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# load the baseline model\n",
        "checkpoint = torch.load(\"./vgg_16_bn.pt\", map_location=torch.device('cuda:0'))\n",
        "model_ori.load_state_dict(checkpoint['state_dict'])\n",
        "\n"
      ],
      "metadata": {
        "id": "R7LQVpYnQzYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93afaec-283b-4e41-d183-57646a7987c1"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(inplace=True)\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu3): ReLU(inplace=True)\n",
            "    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu4): ReLU(inplace=True)\n",
            "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu6): ReLU(inplace=True)\n",
            "    (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu7): ReLU(inplace=True)\n",
            "    (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu8): ReLU(inplace=True)\n",
            "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu10): ReLU(inplace=True)\n",
            "    (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu11): ReLU(inplace=True)\n",
            "    (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu12): ReLU(inplace=True)\n",
            "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu14): ReLU(inplace=True)\n",
            "    (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu15): ReLU(inplace=True)\n",
            "    (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu16): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 3: Evaluate a model**\n",
        "\n",
        "The model is assessed via three\n",
        "dimensions: accuracy, required Multiply Accumulate Opera-\n",
        "tions (MACs), and the number of parameters (Params). The\n",
        "compression ratio (CR) is quantified as the percentage reduc-\n",
        "tion in MACs/Params when compared to the original model.\n",
        "\n",
        "There're many tools to asses the model. In this serie, we use [ptflop](https://github.com/sovrasov/flops-counter.pytorch) to assess the model. Please read [this](https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning) and [this](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d) to understand more."
      ],
      "metadata": {
        "id": "_2pBOleFX13G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating the baseline model:\")\n",
        "_, accuracy_model_ori, _ = validate(val_loader, model_ori, criterion)\n",
        "print(f\"This model's accuracy is {accuracy_model_ori}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfdZoOCaX68u",
        "outputId": "211e4961-6b6c-4e39-b03b-07b2db5265be"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the baseline model:\n",
            " * Acc@1 93.960 Acc@5 99.730\n",
            "This model's accuracy is 93.95999908447266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ptflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amiWLjJEYHYO",
        "outputId": "c99f8604-b025-4505-9f58-14fdbdd372e3"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ptflops in /usr/local/lib/python3.10/dist-packages (0.7.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->ptflops) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->ptflops) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ptflops) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ptflops) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ptflops import get_model_complexity_info\n",
        "with torch.cuda.device(0):\n",
        "  macs, params = get_model_complexity_info(model_ori, (3, 32, 32), as_strings=False, print_per_layer_stat=True, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zta0pSkEX_Ce",
        "outputId": "4a6a5f10-7cb0-4240-de2b-561514e81934"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  14.99 M, 100.000% Params, 314.69 MMac, 99.872% MACs, \n",
            "  (features): Sequential(\n",
            "    14.72 M, 98.207% Params, 314.43 MMac, 99.787% MACs, \n",
            "    (conv0): Conv2d(1.79 k, 0.012% Params, 1.84 MMac, 0.582% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm0): BatchNorm2d(128, 0.001% Params, 131.07 KMac, 0.042% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(0, 0.000% Params, 65.54 KMac, 0.021% MACs, inplace=True)\n",
            "    (conv1): Conv2d(36.93 k, 0.246% Params, 37.81 MMac, 12.001% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm1): BatchNorm2d(128, 0.001% Params, 131.07 KMac, 0.042% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(0, 0.000% Params, 65.54 KMac, 0.021% MACs, inplace=True)\n",
            "    (pool2): MaxPool2d(0, 0.000% Params, 65.54 KMac, 0.021% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(73.86 k, 0.493% Params, 18.91 MMac, 6.000% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm3): BatchNorm2d(256, 0.002% Params, 65.54 KMac, 0.021% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu3): ReLU(0, 0.000% Params, 32.77 KMac, 0.010% MACs, inplace=True)\n",
            "    (conv4): Conv2d(147.58 k, 0.984% Params, 37.78 MMac, 11.990% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm4): BatchNorm2d(256, 0.002% Params, 65.54 KMac, 0.021% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu4): ReLU(0, 0.000% Params, 32.77 KMac, 0.010% MACs, inplace=True)\n",
            "    (pool5): MaxPool2d(0, 0.000% Params, 32.77 KMac, 0.010% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(295.17 k, 1.969% Params, 18.89 MMac, 5.995% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm6): BatchNorm2d(512, 0.003% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu6): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (conv7): Conv2d(590.08 k, 3.936% Params, 37.77 MMac, 11.985% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm7): BatchNorm2d(512, 0.003% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu7): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (conv8): Conv2d(590.08 k, 3.936% Params, 37.77 MMac, 11.985% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm8): BatchNorm2d(512, 0.003% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu8): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (pool9): MaxPool2d(0, 0.000% Params, 16.38 KMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv10): Conv2d(1.18 M, 7.872% Params, 18.88 MMac, 5.993% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm10): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu10): ReLU(0, 0.000% Params, 8.19 KMac, 0.003% MACs, inplace=True)\n",
            "    (conv11): Conv2d(2.36 M, 15.741% Params, 37.76 MMac, 11.983% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm11): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu11): ReLU(0, 0.000% Params, 8.19 KMac, 0.003% MACs, inplace=True)\n",
            "    (conv12): Conv2d(2.36 M, 15.741% Params, 37.76 MMac, 11.983% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm12): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu12): ReLU(0, 0.000% Params, 8.19 KMac, 0.003% MACs, inplace=True)\n",
            "    (pool13): MaxPool2d(0, 0.000% Params, 8.19 KMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv14): Conv2d(2.36 M, 15.741% Params, 9.44 MMac, 2.996% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm14): BatchNorm2d(1.02 k, 0.007% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu14): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (conv15): Conv2d(2.36 M, 15.741% Params, 9.44 MMac, 2.996% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm15): BatchNorm2d(1.02 k, 0.007% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu15): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (conv16): Conv2d(2.36 M, 15.741% Params, 9.44 MMac, 2.996% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm16): BatchNorm2d(1.02 k, 0.007% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu16): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    268.81 k, 1.793% Params, 269.32 KMac, 0.085% MACs, \n",
            "    (linear1): Linear(262.66 k, 1.752% Params, 262.66 KMac, 0.083% MACs, in_features=512, out_features=512, bias=True)\n",
            "    (norm1): BatchNorm1d(1.02 k, 0.007% Params, 1.02 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(0, 0.000% Params, 512.0 Mac, 0.000% MACs, inplace=True)\n",
            "    (linear2): Linear(5.13 k, 0.034% Params, 5.13 KMac, 0.002% MACs, in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of parameter and MACs of this model are {params} and {macs}, respectively.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPGni3I0YfYX",
        "outputId": "6ebf1257-9156-47e4-cdab-ee0f80a795e7"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of parameter and MACs of this model are 14991946 and 315096586, respectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pU7P10nGFBb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 4: 3 methods to prune the model**\n",
        "Install function to evaluate the importance of filters\n",
        "\n",
        "\n",
        "*   Random\n",
        "*   Norm-based\n",
        "*   Distance-based\n",
        "\n"
      ],
      "metadata": {
        "id": "6pFryMvGXX9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compress_rate = [0.25]*13 # prune 25% of all layers\n",
        "model_prune = vgg_16_bn(compress_rate=compress_rate).cuda()\n",
        "print(model_prune)\n"
      ],
      "metadata": {
        "id": "SxxynwaVXbp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11df86da-a8c8-4f6c-f0d0-04e3bc24edf9"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (conv0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(inplace=True)\n",
            "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu3): ReLU(inplace=True)\n",
            "    (conv4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu4): ReLU(inplace=True)\n",
            "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu6): ReLU(inplace=True)\n",
            "    (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu7): ReLU(inplace=True)\n",
            "    (conv8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu8): ReLU(inplace=True)\n",
            "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu10): ReLU(inplace=True)\n",
            "    (conv11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu11): ReLU(inplace=True)\n",
            "    (conv12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu12): ReLU(inplace=True)\n",
            "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv14): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm14): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu14): ReLU(inplace=True)\n",
            "    (conv15): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu15): ReLU(inplace=True)\n",
            "    (conv16): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu16): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
            "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(inplace=True)\n",
            "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 4.1: Random**\n"
      ],
      "metadata": {
        "id": "igXpB54LvK3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_random(model, model_ori):\n",
        "    oristate_dict = model_ori.state_dict()\n",
        "    state_dict = model.state_dict()\n",
        "    last_select_index = None  # Conv index selected in the previous layer\n",
        "\n",
        "    cnt = 0\n",
        "    for name, module in model.named_modules():\n",
        "        name = name.replace('module.', '')\n",
        "\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            cnt += 1\n",
        "            oriweight = oristate_dict[name + '.weight']\n",
        "            curweight = state_dict[name + '.weight']\n",
        "            orifilter_num = oriweight.size(0)\n",
        "            currentfilter_num = curweight.size(0)\n",
        "            print(f\"Processing layer {cnt}, original layer has {orifilter_num} filters, pruning model has {currentfilter_num} filters\")\n",
        "\n",
        "\n",
        "            if orifilter_num != currentfilter_num:\n",
        "                cov_id = cnt\n",
        "                #************ rank the filter's importance here\n",
        "                rank = np.arange(1, orifilter_num + 1)\n",
        "                np.random.shuffle(rank)\n",
        "                #********************\n",
        "                print(f\"rank {rank}\")\n",
        "                select_index = np.argsort(\n",
        "                    rank)[orifilter_num-currentfilter_num:]  # preserved filter id\n",
        "                select_index.sort()\n",
        "\n",
        "                if last_select_index is not None:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        for index_j, j in enumerate(last_select_index):\n",
        "                            state_dict[name + '.weight'][index_i][index_j] = \\\n",
        "                                oristate_dict[name + '.weight'][i][j]\n",
        "                else:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        state_dict[name + '.weight'][index_i] = \\\n",
        "                            oristate_dict[name + '.weight'][i]\n",
        "\n",
        "                last_select_index = select_index\n",
        "\n",
        "            elif last_select_index is not None:\n",
        "                for i in range(orifilter_num):\n",
        "                    for index_j, j in enumerate(last_select_index):\n",
        "                        state_dict[name + '.weight'][i][index_j] = \\\n",
        "                            oristate_dict[name + '.weight'][i][j]\n",
        "            else:\n",
        "                state_dict[name + '.weight'] = oriweight\n",
        "                last_select_index = None\n",
        "\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "BzcvypUwFx8b"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_random(model_prune, model_ori)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYPThKmdgioW",
        "outputId": "63c5517f-4ef4-44d5-a006-2c7e47c15a7b"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing layer 1, original layer has 64 filters, pruning model has 48 filters\n",
            "rank [13 55 38 40 21  1 33 43 51 58 28  9  2 53 12 20  3  5 52 48  4 54 26 35\n",
            " 34 41 25 50  7 27 17 64 56 22 15 45 59 19 32 36 63  8 39 62 30  6 49 10\n",
            " 44 37 46 31 61 42 24 57 47 18 14 29 60 16 23 11]\n",
            "Processing layer 2, original layer has 64 filters, pruning model has 48 filters\n",
            "rank [57  7 34 27 46 33 29 13 35 49 40 55 45  8 32 56 25 10 47 42 48 50 41 38\n",
            " 60 11 18 31 14 22 62 16 51  3 28 19 52  9 63 23 44 17 64 59 21 15 37 54\n",
            " 20 36 61  6 53  4 24 58 30  1 12 39 43  5  2 26]\n",
            "Processing layer 3, original layer has 128 filters, pruning model has 96 filters\n",
            "rank [ 21  70  89 107  46  57  87 111   6  58  67   5  80  83  47 115  43 108\n",
            "  16  93  85 109  56   2 125 120  79 113  82  48 110 128  53   8 123  18\n",
            "   4  51  33  17  38  29   1  36  25  52 104  22  11  66 101  34  95  98\n",
            "  81  23  59 122  94  30  60 112 121  63  76 126 119  69  27  78  32 105\n",
            "  74  90  64  41  97  14  54 114  65  55 103  24   7  61 100  19  68   3\n",
            " 127  35  62  73  75  31  84  10  77 102  40 116  96  20  42  92  86  91\n",
            "  88 124 106  37  26  99  28  45  13 118  72  15 117  39  71  50  49  12\n",
            "   9  44]\n",
            "Processing layer 4, original layer has 128 filters, pruning model has 96 filters\n",
            "rank [ 68  56  59  35  76  89  23  73  62  38  65  72  51 128  69 106 100  50\n",
            "  30  95  63  93  99  29  20 102  40   9  47  22 125  92  15 116  98  13\n",
            " 110  82  31  64  21   4  97 121 111  79  46 108  74  58  44  49  45  77\n",
            " 124 122  66  61  28   7   6 123 118  54  85  87 119 107  67  36 101 127\n",
            "  11  43 112  12  42  52   8  84  75  14  48  80 117 126  91  10  27  33\n",
            "  17  41 114  24   1   5  26 120  32  70   2   3 115  57  96 105 113  55\n",
            "  19  90  86  16  25  81  83  88  18  94 104  37 103 109  34  71  53  39\n",
            "  60  78]\n",
            "Processing layer 5, original layer has 256 filters, pruning model has 192 filters\n",
            "rank [226   1  46  32 223 233  26   9  71  88 218 212 101  90  42 214 102 175\n",
            " 247  57 198 124 131 231 204 144 238  41  83  75  79 100 150 232 145 202\n",
            " 161   7  29 219 170  43  59 107  44  27 155 113 192 240  51 213 118  62\n",
            "  72  95  65  91 171 147  87 128 121 183 237  73 180  76  25 146 134  36\n",
            "  22  63 168  61  55 184 132 215 201 110 133  74 105  40   2 199 205 166\n",
            "  69  38 104 125  84 248 220 217 138 108   8  16 179  18 178 158  92  34\n",
            "  67 152 119 242  35  98  97  96  50 167  21 173  10 142 141 174 245 136\n",
            " 227  86 137 221  13 208  37 165 253  80 112 255  64 129 207 239 154 103\n",
            " 209  53 169 236 254  14 115 111 186 139 114 187 210 120 235 176  58  12\n",
            " 153 193 116  70 130 250 151  60 185 143  30  93 140  45 234 241 172 206\n",
            " 122  77  20 177 188  99 159  48 224 256 123 160 216  54 195  52 211 200\n",
            " 244  66  89  33 194  24  81  11   4 252 246  85  19 190 126  15 135  47\n",
            " 251  31  28 127 162 230 249 229 148  56  94 106 157  23 149  17  39 189\n",
            " 181 228   3 203  49 163 222 191 197 196   5 225 117 243 182 156  78 109\n",
            " 164   6  82  68]\n",
            "Processing layer 6, original layer has 256 filters, pruning model has 192 filters\n",
            "rank [177 158 135  40 143  90 136  56   6 115 250 124 202 178  68  64 209  45\n",
            "  78 140  77 230 107  35  13 155 238 198 161  29 204 248 197 133  80 185\n",
            " 213 151  61 103 220  97 109  82 227 235 254 176 147 217 164 100  67 246\n",
            "  31  54   3  21  84  14 192  75 193 215 127 200 113  26 116 214 249 242\n",
            "  87  73  32 187 129 243 174 106   4 167  24  30 211  52  92 216 165 131\n",
            " 207 102 224 191 175 138 173 234 153 219  38 123  71 201 247  74   2 231\n",
            "  47  63  69  46 180 184 104  94 221 255 226  39 239 228  83  28 122 256\n",
            " 163  43 169 240  36  48  76 152 203 237 170  10 137 218 105 142   9 241\n",
            "  27 149  96  17   7 108 146  23  34  81 156  18  51 208 171 145 130  70\n",
            " 212  57  95 126 189  91  89 232 112 132 118 251 119  44 229  85 168 186\n",
            "  11 179  25  60 144   5 252  49  72  16 159 111 160  37 199 244 166 110\n",
            " 188 148  19 120 233 162 222 139  93 194 210  53 181 196 223 134 245   1\n",
            "   8  58 121  86 150  33 195 117  22  62 206 157 128  41  12 205 141 225\n",
            " 125 236 172  15 183 114 154  98  55  59  42 253  50  88 182  79 190  20\n",
            "  65 101  99  66]\n",
            "Processing layer 7, original layer has 256 filters, pruning model has 192 filters\n",
            "rank [177  46 138 203 141  93 200 202 249  57 232  85 204 164 189  32 126 183\n",
            " 219   7 231  89 135  56 160  42 153  10  76 205 146  88 237 229  35 184\n",
            "  64 194  60  43 149  87 102 216   1 192  45 171  54  15 122 233  71 123\n",
            " 112 103  16 168 180 162 197 227 224 222 248 218 143  47 246  97 179  94\n",
            " 213 187  12 188 191 228 137 236 152 145 139  98 173  61 181 243 212  31\n",
            "  81 170  39 157  28  75  78 151  95 132 130  67   4 239  25 235 131 150\n",
            " 174 215  34  62  69  79  14 207 163  41  38   9 100  51 209 253 175  52\n",
            " 159 244  33 120 211 226  96 136  18 110 134  13 190  29 199 254 208  72\n",
            "  99 178  37 127  17   8 128 193  59 251  48  20  74  68   6 230 117 220\n",
            " 119 121 176 245 250 169 198  91  65 125 186  27 252  50 156 129 217 161\n",
            "  19 165 142  55  80  49 133 225 155  90 210 111 167 158  84 124 201 182\n",
            "  40 106  11 166  77  66 108   2  26 148  36 223 206   5 234 242 140 241\n",
            " 116 114  24 185  22 105  86 154 196 147 113 118  83 101  53  58 109 221\n",
            "  23 144 107 195  70 256  30 214  82 255  73  44  63 115 240   3  92 247\n",
            " 172 238 104  21]\n",
            "Processing layer 8, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [299 292 280 343 173 500 114 362 161 480 138   7 210 139 228 390 168  48\n",
            "  65 281 231 330 145 188 457 322 489 455 420 380 402 160  76 101 242 424\n",
            "  72 243 425 184 209 307 412 311 298  37 259 354 277 131 486 115 487 119\n",
            " 218 123 502 177  90 387 182 394 122 490  46  61 478 213  21 417   8 341\n",
            " 255  53 504 193 423 346 399 310  42 151   6  43 260 154  44 166 264 205\n",
            " 283  20  23 106 448 287 297 236 511 395 337 505 235 282 223 415 358 363\n",
            " 194  67  81 155 472 450 482 224  85 393 309 400  24  35 300 414 159 410\n",
            " 257 397 355   3 473 368 294  31 385 508  52 278  10 430 143  59 108 112\n",
            " 426 332 360  25  41 506 118 234  69  47 285 427  49  28  57 233 325 232\n",
            " 130 128 246 286 367 208 440 237  51 312 327 225  99 290 499 432 493 202\n",
            " 102 350 291 320 195 317 247  13  92 200 512 431   2 449 316 484 413  96\n",
            " 116 254 464 220 258 469 339  79 353 475  33 248 466  39  11 270 479 262\n",
            " 338 347  18 496  98 124  68 269 133 120 174 507 429 198 396 187 167 361\n",
            "  73 146 140 326 408 483 180 284 163 323  87 221 207 289 454 422 401 103\n",
            " 389 250 375 296 421 443 126   5 135 191 272 437 364  27 409 324 113  94\n",
            "  16 406 219  75 381 176 293  34 498 302 251 170 373 222 276 304 111 501\n",
            " 365 199 306 428 398 404 204 150  60  83 491 370 476 227 485  50 239 433\n",
            " 261  36 215 458 104 434 407 181 497  64 471 465 229 271 467 148 369 121\n",
            " 107 463 315 460 117 172 169 411 249 245 416 356 313 158 197 378 388 453\n",
            " 446  17 374 253 462 477 359 314 386 377 441 216 156 175  19 190 468 351\n",
            " 263  82 274 333 241 445   1 344 447 105 201  74 256 238 164 137 335  84\n",
            " 345 303 275 279  58  56  62 226  91 392 451 203 334 147 488 305 383 273\n",
            " 268 418 352 331 459  55 419 211 165  22 214 157 110 474  66 403  80 217\n",
            " 212 109 178 357 141 366 481 295 348   9 321  54 252  40  71 509 382 100\n",
            " 183 265 438 379  26 371 319 152 503  78 192 510 308 206 495 288 452 328\n",
            "  63 179 244 435  30  88 125 134 149  95  70 372 301  29 267 329 494 132\n",
            " 376 185 230 461  93 405 384 129  77  86  32 340 318 186 442  89 336 439\n",
            " 456  12 153 470 127 444 342  38 142 171  45 492 196 136  14 162 436  15\n",
            " 266 144  97   4 189 349 240 391]\n",
            "Processing layer 9, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [161 307  15   6  57 155 112 116  20 392 246 113 354 335 207 132  80 456\n",
            " 309 221  48 478 483 506 410 172 102   5 148 420 403 171 118 261 205 385\n",
            " 300  36 141 327 430 250 388  71  84 108 248 159 442  35 294 493  52  82\n",
            "  76 226  89 511 459 311  97 325 401 446 145 448 280   3 443 272 201 365\n",
            " 206 326 336 402 457  68 497 496 418 473 356 464  86 135 324 421 163 437\n",
            " 231 245 360  23 146 469 479 217  53 425 143 433 104  37 241 460 262 390\n",
            " 175 409 258 477 255  66 152 164 197  93  29  11  79 133 190 369  58 362\n",
            " 267  90 195 395 212 291 475 288 359 468 251 377 429 169 203  47 346 366\n",
            " 349 198 352 384 115 310 188  55 162 482 319 510 340 109 211  70 508 472\n",
            "  16 313  63 387  25 413 357 269  14  50 103 290 242 491 278 281 168 484\n",
            " 128 487 111 467 492 368 265 138 379 304 367 144 351 348 233 244  60 338\n",
            " 270 200 105 376 507 471 156 495 345 193 153 120 382 204 254   9 504  78\n",
            " 434 264 386 306 328 158 256  27 295 106   1 396  17 218 499 185 166 407\n",
            "  28 273 173 416 139   2  40 408  59   4 439 405 391 114 236 149 101 488\n",
            " 355  85 289 225 454 427 361 274 199 178  61 353 486 121 333 312 305 136\n",
            " 431 494 373 372 119 512 282 480 157  30 316  43 375  67 398 252 323  81\n",
            " 450 414 461 187 237 423 210 127 334 302 167 308 329 286  24 234 276 122\n",
            " 381  31 181 406 393  39 150 417  73 412 184 331  95 490   7 422 196 424\n",
            " 370 428 230 299  33 509 341 266 343 453  88 177 170  83 458  69 415 238\n",
            " 243 182 440  96 397 449 202 296 179 100  41 208 438 318  18 344  77 455\n",
            "  10 160 337 214 364 117 183  74 176 481 400 502 444 140 131 219 192 130\n",
            " 126 284 380 228 154  75  64 500 292 151  54 320 227 147 303 383 191  12\n",
            " 239  26 123 399 501 498 253 465  19  56 215 224  65 271 194  99  46 452\n",
            " 275  42   8 436  49 285 315 229 474  87  34 358 134 476 174 249 317 247\n",
            " 137 394 445 441 216 432 503 322  51 419 213 363 447 435 404 283  98  44\n",
            " 287 489 180 374 263 125 321 389  45 223  72  62 220 222 330  22 165 485\n",
            " 259  21 268  13 260 342 297 186 279 347  32 378 298 189 257 209 462 451\n",
            " 110 293 235 129 301 505 277 466 124  38 339  91 463 426 350 142 411 332\n",
            " 371 107 314 232  94  92 470 240]\n",
            "Processing layer 10, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [311 105 344 103 367  80 208 464 379 346 497 414 235 160   9 340 463 380\n",
            " 151 364 486 230 162 273 375 413 241  86 174 283 330  97 315 322 326 372\n",
            " 257 336 447 294 327 359 488 278 303 132 503 215 479 106  24 186 128  25\n",
            " 453 118 360 498 459 334 250 357  33 146 137 258  58 342 256 119 213 279\n",
            " 219 108 254   2 419 423 197 107 196 173 348 472 500 502 296 481 292  61\n",
            "  67 253  76 424 446 384 288 501  74   4 138 255 352 496 136 309 188 390\n",
            " 226 386 428   1  27 444 194 478 269 467  45 388 261 280 382 282 427 345\n",
            "  40 189 100 272  52 441 185 134 304 489 482 265 224 245 110 430 328 451\n",
            " 217 324  23  48  16 231 483 242 405 240  29 491 452  83 121 412 236 396\n",
            " 155 112  65 484 307 325   7 287 450 198 220 163  22 314 349 460 126  84\n",
            " 109  62  81  36  71 457 510 225 462 393 154 124 271 190  31  82 293  30\n",
            " 113 407  73 222  85 416 404 320 499 465 434 286 432 431 421 221 139 264\n",
            " 420 350 168 512  10 347 477 156 153  12 175 485  34 456 123 141 218 276\n",
            " 122 362  37 466 120 358 474 409 339 281 468 442 243 439 177 403  70 425\n",
            " 313 461 205 408 259 195  64 237  38 114 351  99 228 406  35 437 223 310\n",
            " 473 429 319 169  39 149 129 157  78 415 116  15 308 458 332 395 209 507\n",
            " 158  44 400 207 438 410 418 142  19 470 504 192 115 366 333 383 170 448\n",
            " 200 211 373 435 246  59 341  66 238 117 201 268 216 361 509 445 297 291\n",
            " 433 443 355 248  77 299 266 277 365 317 455 353 263 127 394 305 252 440\n",
            " 335 239 140 212 125 449 229  41 171 289 387 202   8 495 193 233  96 323\n",
            " 180 249  54 181  20 490 371 204  91  93   5 389 508 399 210 493  72 182\n",
            " 234 354 385 104 184 247 111  69 214  50 391 302 312 165 368 131  63 471\n",
            " 133 145 511 101 370 150  90  18 401 392 274  32  87 166  57 183 301 130\n",
            " 232 398  42 135 159 318  79 422 436 363 102 454 487 369 505 306 285  11\n",
            " 298  53 270  94  17   3 251 411 187  95 321 144 329 275 164 506 295  75\n",
            " 203 377  89 476 381 376 178 492  28 426 172  60 260  21 176 167 179 374\n",
            " 469 244  56  98  49 284 199 290  26  51 152  46 206 338 417 161 316 475\n",
            "  14 191 337 147 480 262 227 331  68 397   6  47 378 343  13 402 356  92\n",
            " 148 143 300  88  43  55 267 494]\n",
            "Processing layer 11, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [345 227  76 166 302 122 383  23 453  59  10 335 195 431 393 220 362 224\n",
            "  91 276 450 215 468 245   8  66  73 395 313 352 176 441 306 238 221 314\n",
            " 103 487 357 168 452  44  51 353 234 425 512 289 117 169 445 342 211 305\n",
            " 489 459  28 451 203  48 378  70 241 414  88 501  42 229 158 277 504 194\n",
            "  93 336 109 315  13 160 326 400 136 254 437 449 405 464 511 125 187 287\n",
            " 233  32 338 350  64 309 488 252 256 197 295  11 323 154 204 288 421 282\n",
            " 212  33 216 404  38  55 301 201 120 129 128 108 235  29 280 174 491  97\n",
            " 114  83 462 153 321 255  69 286 455 146 257  67 374 351  60  16 435 392\n",
            " 205 284  82 330 409 279 385 298 328 407 329 493 369 140 333 457 156 496\n",
            " 199 307 135   3 510 415 152 456 159 182  89 505 243 317 164 106 492 119\n",
            " 202  36  24   9 219 494 444 192 236 322 185 141 331 200 430 258 259 242\n",
            " 250 432  31 150 448 283  15 419 191 428 346 124   2 394 207 178 293 189\n",
            " 223 418 375 417   1 225 304  19 264 341 427  98   6 123  37 149 281  95\n",
            " 461  63 349 278   5 360  80 208 171 363 101  78 179 509 372 482 332 272\n",
            " 228 500 490 397 296 132 429  43 300 163 247 465 113 403 454 506 271  62\n",
            " 213 382 107 381 131 373 422 358 377 269 142  50  47 246 384 292 111 133\n",
            " 261 416 440 319 325 100 167 458  87 483 147  58 230 361  52 263 339 134\n",
            " 391 127  71 218 399 102 267  96  12 502 184 348 275 438 294 420 365 237\n",
            "  77 324 105 410 308  75 486  72 137  49 148  61 265 226 112 472 262  54\n",
            " 389 447 426 398 214 181 401 497 412 121 170 473 480 408 161 273  90 251\n",
            " 138 479 240  79 297 320  57 222 439 210 475 274 442 477 411 508 367  40\n",
            " 116 396 180  20 498 268 366 371 139  18 484 478 436 413 340  81 316  21\n",
            " 183  27  86 249  17  53  68  34 347 130 343 173 162 311 145 463 434 126\n",
            "  74  22  25 253 476  30 110 231  85 190 209 370 186 356 364 337 470 175\n",
            " 507  65  45 327 390 217   7 291 495  92 188 312 359 266 115  84 387 172\n",
            " 290 157 177 118 368 481 143 104 485  26 469 446 285 423 193  39  41 503\n",
            " 443 165 467 299 198 354 376 244 310  35 380 474 379 260 334 466   4 206\n",
            " 318 151  46 344 388  99 303 433 499 196 406 248  94 402 155 144 355 386\n",
            " 239 232  14 460  56 471 270 424]\n",
            "Processing layer 12, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [285 209 179 279 132 297 165 488 173 175 351  23 266 365  99 214 393 344\n",
            " 265 116  25 323 242 181 104 144   2  11  50 392 249  75 114 286 452 448\n",
            " 456 490  61 331 333 264  27 243   5 387  24 169  68 427 238 453 252 406\n",
            " 482  10 227 511 307 416 504 502 368 358  66 234 425  85 486 275 474 126\n",
            " 489  45  46 294  43 176 117  60 397 268  70 362 400 263 193  51 464 329\n",
            " 109 342 236 258 475 143 110 408 220 501 493 184 247 187  29 313 316 311\n",
            " 222 303  74 472 195 256  95  30 508 206  64 352 321 443 496 171 405 158\n",
            " 239   6 507 133 479 442 295 210  73 492 135 432  98  36 248 396   3 395\n",
            "  32 334 299 337  12 253 130  89 112 310 457 494 412  19  53 149 190 308\n",
            " 274 124 255 437  33 487 111 215 440 446  72 422 118 273 354 384 371 381\n",
            " 106 409 340 121 113 398 168 444 282 420 280 191 301 232 374 361 449 174\n",
            "   1 188 436 380  67 289 483 270 402  59 157  92 205 207 470 320 369 309\n",
            " 105 355   9 137 455 478 136 269 302  55 434  21 231 328 278 254 386 235\n",
            " 447 200 336 359 138 454 382 426 115  57 394  91 375  82 423 276  65 471\n",
            " 287 399  96 250 131 221 202 330  34 403 411 429 122 251 211 178 473  93\n",
            " 421 100 469  49  71 103 219  56   7 460 185  79 383 223 441 203 160 102\n",
            " 428 401 413 154 332 290  41 267 389 261 376 360 296 467 237 305 306  83\n",
            " 430 156 315 262 419 477 119 292 246 484  81 194 291 155 288 314 312 378\n",
            " 128 226 325  69 216 407  76 139  54 465 353 410 499 343 476 241 245 451\n",
            " 485 431  14 108 445  80  18 372  94 417 338 120  52 159 500 141 197 491\n",
            " 450 319 148 166 180 506  42  90 433  58  17 357 277 145 480  84  26 373\n",
            " 271  39  97 458 512 468   8 495 364  28 101 213 240 367 385 212 339 127\n",
            " 164 161 503 363  47 192 244 228  62 177 497  44 140  63 356 414 459 379\n",
            "  48 391  35  77 390 346 123 259  16 163 186 350 341 326 107 225 208 345\n",
            "  86 230 217 498  20 218 150 233 151  88 167 424 318 142 183 199  87 481\n",
            " 129 189 438 284 462  38 201 439 509 335  15  13 510 304 224 162 204 404\n",
            " 317 272 146  31 327 134 349 198 153 370 196 348 298  37 418 347 170 435\n",
            " 461 293 125 147 182 152 172  40 300 366 281 257 505  22 322 388 377 415\n",
            "   4 463 229 283 260 466  78 324]\n",
            "Processing layer 13, original layer has 512 filters, pruning model has 384 filters\n",
            "rank [296 234 503  36 302 321 192 224   1 120 335 462  41  79 101 352 424  80\n",
            "  43 496  87 202 275 217 292 306 388 394 176  70 434 152 469  62  74 396\n",
            " 200 129 293 185 461 369 430  27   6  31 266 504 178 442 334 397 319 403\n",
            "  77 248 339 359 246 174 301 439  66 283 473  84 215 509  90 322 243 375\n",
            " 256 406 289 103 166 365 264 450 511 108  13 158  91  94 458 196 342 230\n",
            "  71 262  11  57 305 182 125 314 372 233 269 392 153 362  40 324  54  72\n",
            " 491 304 197   8 148 484 477  89 501 280  81 507 412 483 151 271   4 147\n",
            "  24 485  25  58 482 273   2 345 268 138 295 229 210 144 260  47 451 393\n",
            " 168 159 317 358 254 341 476 373 223 290 386  97 143 124 370 411 194  28\n",
            " 276 436 420  34 247 187 381 475 226 353  49 498 105 261 320 227  65 213\n",
            " 179  14 415  22 287 338 330 385 449 111 413 134 118 239  73 216 156 401\n",
            " 206 265  92 291 378 348 336 326 294 205 126 169  51 481 508 456 263 495\n",
            " 107 130  37  98  29 329 270 150 512 325 211  48 347 175 161 444 279 377\n",
            " 494  53 480  75 149 404 400 201 328 186 207 236 429  64 315  68 399 145\n",
            "   3  38 447 346 193 445  20 189  96 510 104 219 238 486 114  56 350 390\n",
            "   5 380 448 437 141 191 199 487 172 327 135 493 188  83 288 123 232 203\n",
            "  95 441 237 467 102 250 154 374 446 368 122 221 474 421 443  67 423 272\n",
            " 278 318 109 171 311  45 463 435 490 113 282 222 198  52 137  17 255 405\n",
            " 502 146 488  63  35  50 285  10 457 252 281  26 117 506 367 297 357 422\n",
            " 165  69 242 337 363 454 479 115 371 409 416 361  46  99 466 249  42 209\n",
            " 258 452 244 433 343 312 286 267 180 500 190 472 208  85  93 465 344 408\n",
            " 257 389 139  86 455 382 307  30 251 245 300 431 299 499 460 351 332  76\n",
            " 387 112 340 384 410  16 497 106 505 379 253   7 195 308 284 204 349 259\n",
            " 331  61 360  15 231 214 119 110  33 157 309 391 489  59 225  44 164  88\n",
            " 438 426 316 220 170 366 277 235 183 432 323 468 428 407  23  12 398 128\n",
            "  55 459 478 212 155 355 376 100 492 218 464 133 181  82  32  39 417  18\n",
            "  19 402 173 414 418  60 354 240   9 140 167 364 440 116 313 121 136 383\n",
            " 162 177 333 425 298 142 163 303  78 395 241 184 132 131  21 127 453 160\n",
            " 427 356 228 470 471 310 274 419]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune(model_prune, train_loader, val_loader, epochs=1, criterion=criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr2GM056jNaX",
        "outputId": "df84ddd3-f9c6-4bda-bd3b-0487e2309b9d"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 10.000 Acc@5 50.000\n",
            "learning_rate: 0.0001\n",
            "Epoch[0](0/391): Loss 2.3605 Prec@1(1,5) 3.91, 49.22 Lr 0.0001\n",
            "Epoch[0](39/391): Loss 2.0935 Prec@1(1,5) 28.22, 73.89 Lr 0.0001\n",
            "Epoch[0](78/391): Loss 1.8369 Prec@1(1,5) 45.62, 82.97 Lr 0.0001\n",
            "Epoch[0](117/391): Loss 1.6610 Prec@1(1,5) 52.67, 86.84 Lr 0.0001\n",
            "Epoch[0](156/391): Loss 1.5287 Prec@1(1,5) 57.10, 89.17 Lr 0.0001\n",
            "Epoch[0](195/391): Loss 1.4258 Prec@1(1,5) 60.11, 90.74 Lr 0.0001\n",
            "Epoch[0](234/391): Loss 1.3396 Prec@1(1,5) 62.58, 91.85 Lr 0.0001\n",
            "Epoch[0](273/391): Loss 1.2704 Prec@1(1,5) 64.42, 92.69 Lr 0.0001\n",
            "Epoch[0](312/391): Loss 1.2099 Prec@1(1,5) 66.10, 93.39 Lr 0.0001\n",
            "Epoch[0](351/391): Loss 1.1588 Prec@1(1,5) 67.49, 93.93 Lr 0.0001\n",
            "Epoch[0](390/391): Loss 1.1152 Prec@1(1,5) 68.62, 94.37 Lr 0.0001\n",
            " * Acc@1 78.740 Acc@5 98.300\n",
            "=>Best accuracy 78.740\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu3): ReLU(inplace=True)\n",
              "    (conv4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu4): ReLU(inplace=True)\n",
              "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu6): ReLU(inplace=True)\n",
              "    (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu7): ReLU(inplace=True)\n",
              "    (conv8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu8): ReLU(inplace=True)\n",
              "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu10): ReLU(inplace=True)\n",
              "    (conv11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu11): ReLU(inplace=True)\n",
              "    (conv12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu12): ReLU(inplace=True)\n",
              "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv14): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm14): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu14): ReLU(inplace=True)\n",
              "    (conv15): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu15): ReLU(inplace=True)\n",
              "    (conv16): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu16): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
              "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.device(0):\n",
        "  macs_prune, params_prune = get_model_complexity_info(model_prune, (3, 32, 32), as_strings=False, print_per_layer_stat=True, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XlC_VuUr-CL",
        "outputId": "e7ca924a-5f4e-4669-ba7a-a41428e71e4b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  8.49 M, 100.000% Params, 177.63 MMac, 99.831% MACs, \n",
            "  (features): Sequential(\n",
            "    8.28 M, 97.605% Params, 177.43 MMac, 99.716% MACs, \n",
            "    (conv0): Conv2d(1.34 k, 0.016% Params, 1.38 MMac, 0.773% MACs, 3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm0): BatchNorm2d(96, 0.001% Params, 98.3 KMac, 0.055% MACs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu0): ReLU(0, 0.000% Params, 49.15 KMac, 0.028% MACs, inplace=True)\n",
            "    (conv1): Conv2d(20.78 k, 0.245% Params, 21.28 MMac, 11.961% MACs, 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm1): BatchNorm2d(96, 0.001% Params, 98.3 KMac, 0.055% MACs, 48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(0, 0.000% Params, 49.15 KMac, 0.028% MACs, inplace=True)\n",
            "    (pool2): MaxPool2d(0, 0.000% Params, 49.15 KMac, 0.028% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(41.57 k, 0.490% Params, 10.64 MMac, 5.981% MACs, 48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm3): BatchNorm2d(192, 0.002% Params, 49.15 KMac, 0.028% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu3): ReLU(0, 0.000% Params, 24.58 KMac, 0.014% MACs, inplace=True)\n",
            "    (conv4): Conv2d(83.04 k, 0.978% Params, 21.26 MMac, 11.947% MACs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm4): BatchNorm2d(192, 0.002% Params, 49.15 KMac, 0.028% MACs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu4): ReLU(0, 0.000% Params, 24.58 KMac, 0.014% MACs, inplace=True)\n",
            "    (pool5): MaxPool2d(0, 0.000% Params, 24.58 KMac, 0.014% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(166.08 k, 1.957% Params, 10.63 MMac, 5.974% MACs, 96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm6): BatchNorm2d(384, 0.005% Params, 24.58 KMac, 0.014% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu6): ReLU(0, 0.000% Params, 12.29 KMac, 0.007% MACs, inplace=True)\n",
            "    (conv7): Conv2d(331.97 k, 3.911% Params, 21.25 MMac, 11.941% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm7): BatchNorm2d(384, 0.005% Params, 24.58 KMac, 0.014% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu7): ReLU(0, 0.000% Params, 12.29 KMac, 0.007% MACs, inplace=True)\n",
            "    (conv8): Conv2d(331.97 k, 3.911% Params, 21.25 MMac, 11.941% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm8): BatchNorm2d(384, 0.005% Params, 24.58 KMac, 0.014% MACs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu8): ReLU(0, 0.000% Params, 12.29 KMac, 0.007% MACs, inplace=True)\n",
            "    (pool9): MaxPool2d(0, 0.000% Params, 12.29 KMac, 0.007% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv10): Conv2d(663.94 k, 7.822% Params, 10.62 MMac, 5.970% MACs, 192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm10): BatchNorm2d(768, 0.009% Params, 12.29 KMac, 0.007% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu10): ReLU(0, 0.000% Params, 6.14 KMac, 0.003% MACs, inplace=True)\n",
            "    (conv11): Conv2d(1.33 M, 15.640% Params, 21.24 MMac, 11.937% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm11): BatchNorm2d(768, 0.009% Params, 12.29 KMac, 0.007% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu11): ReLU(0, 0.000% Params, 6.14 KMac, 0.003% MACs, inplace=True)\n",
            "    (conv12): Conv2d(1.33 M, 15.640% Params, 21.24 MMac, 11.937% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm12): BatchNorm2d(768, 0.009% Params, 12.29 KMac, 0.007% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu12): ReLU(0, 0.000% Params, 6.14 KMac, 0.003% MACs, inplace=True)\n",
            "    (pool13): MaxPool2d(0, 0.000% Params, 6.14 KMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv14): Conv2d(1.33 M, 15.640% Params, 5.31 MMac, 2.984% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm14): BatchNorm2d(768, 0.009% Params, 3.07 KMac, 0.002% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu14): ReLU(0, 0.000% Params, 1.54 KMac, 0.001% MACs, inplace=True)\n",
            "    (conv15): Conv2d(1.33 M, 15.640% Params, 5.31 MMac, 2.984% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm15): BatchNorm2d(768, 0.009% Params, 3.07 KMac, 0.002% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu15): ReLU(0, 0.000% Params, 1.54 KMac, 0.001% MACs, inplace=True)\n",
            "    (conv16): Conv2d(1.33 M, 15.640% Params, 5.31 MMac, 2.984% MACs, 384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (norm16): BatchNorm2d(768, 0.009% Params, 3.07 KMac, 0.002% MACs, 384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu16): ReLU(0, 0.000% Params, 1.54 KMac, 0.001% MACs, inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    203.27 k, 2.395% Params, 203.79 KMac, 0.115% MACs, \n",
            "    (linear1): Linear(197.12 k, 2.322% Params, 197.12 KMac, 0.111% MACs, in_features=384, out_features=512, bias=True)\n",
            "    (norm1): BatchNorm1d(1.02 k, 0.012% Params, 1.02 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu1): ReLU(0, 0.000% Params, 512.0 Mac, 0.000% MACs, inplace=True)\n",
            "    (linear2): Linear(5.13 k, 0.060% Params, 5.13 KMac, 0.003% MACs, in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 4.2: Norm**"
      ],
      "metadata": {
        "id": "cDJ03cH7vShO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_norm(model, model_ori):\n",
        "    oristate_dict = model_ori.state_dict()\n",
        "    state_dict = model.state_dict()\n",
        "    last_select_index = None  # Conv index selected in the previous layer\n",
        "\n",
        "    cnt = 0\n",
        "    for name, module in model.named_modules():\n",
        "        name = name.replace('module.', '')\n",
        "\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            cnt += 1\n",
        "            oriweight = oristate_dict[name + '.weight']\n",
        "            curweight = state_dict[name + '.weight']\n",
        "            orifilter_num = oriweight.size(0)\n",
        "            currentfilter_num = curweight.size(0)\n",
        "            print(f\"Processing layer {cnt}, original layer has {orifilter_num} filters, pruning model has {currentfilter_num} filters\")\n",
        "\n",
        "\n",
        "            if orifilter_num != currentfilter_num:\n",
        "                cov_id = cnt\n",
        "                #************ rank the filter's importance here\n",
        "                print(oristate_dict[name + '.weight'].shape)\n",
        "                weight = oristate_dict[name + '.weight'].data\n",
        "                weight = weight.reshape(weight.size(0), weight.size(1)*weight.size(2)*weight.size(3))\n",
        "                norms = torch.norm(weight, dim=1)  # Compute norm along dimensions 1, 2, and 3\n",
        "                print(norms)\n",
        "\n",
        "                # Now, let's rank them based on the norms.\n",
        "                # We'll get the indices that would sort the norms in descending order.\n",
        "                sorted_indices = torch.argsort(norms, descending=True)\n",
        "\n",
        "                # Print the ranks and corresponding norms\n",
        "                for rank, index in enumerate(sorted_indices):\n",
        "                    norm_value = norms[index]\n",
        "                    # print(f\"Rank {rank + 1}: Norm = {norm_value.item()}\")\n",
        "\n",
        "                # If you also want the indices of filters in descending order of their norms\n",
        "                # print(\"Indices of filters in descending order of their norms:\")\n",
        "                # print(sorted_indices)\n",
        "                rank = sorted_indices.cpu().numpy()\n",
        "                #********************\n",
        "                print(f\"rank {rank}\")\n",
        "                select_index = np.argsort(\n",
        "                    rank)[orifilter_num-currentfilter_num:]  # preserved filter id\n",
        "                select_index.sort()\n",
        "\n",
        "                if last_select_index is not None:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        for index_j, j in enumerate(last_select_index):\n",
        "                            state_dict[name + '.weight'][index_i][index_j] = \\\n",
        "                                oristate_dict[name + '.weight'][i][j]\n",
        "                else:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        state_dict[name + '.weight'][index_i] = \\\n",
        "                            oristate_dict[name + '.weight'][i]\n",
        "\n",
        "                last_select_index = select_index\n",
        "\n",
        "            elif last_select_index is not None:\n",
        "                for i in range(orifilter_num):\n",
        "                    for index_j, j in enumerate(last_select_index):\n",
        "                        state_dict[name + '.weight'][i][index_j] = \\\n",
        "                            oristate_dict[name + '.weight'][i][j]\n",
        "            else:\n",
        "                state_dict[name + '.weight'] = oriweight\n",
        "                last_select_index = None\n",
        "\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "4jNru1MUkyzo"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_norm(model_prune, model_ori)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh5Ed1yTlHOw",
        "outputId": "be1ab6d2-d025-4658-f784-c4e38f303303"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing layer 1, original layer has 64 filters, pruning model has 48 filters\n",
            "torch.Size([64, 3, 3, 3])\n",
            "tensor([1.5891e+00, 8.8759e-01, 5.5363e-01, 3.3376e-01, 2.2699e-01, 7.8806e-01,\n",
            "        3.8824e-01, 3.9319e-01, 3.0613e-02, 6.7065e-01, 3.2999e-01, 1.5340e+00,\n",
            "        4.5140e-02, 5.1688e-04, 1.4907e-01, 2.3611e-01, 1.9442e-01, 1.1847e+00,\n",
            "        2.6520e-01, 1.0161e-01, 2.3032e-01, 6.9368e-01, 1.2299e-01, 1.0715e+00,\n",
            "        5.8923e-01, 5.8216e-01, 2.8645e-01, 3.3000e-01, 1.6501e+00, 1.1890e+00,\n",
            "        9.7305e-01, 5.8581e-01, 1.9635e+00, 7.2750e-04, 1.9415e-01, 3.9707e-04,\n",
            "        6.7574e-01, 1.8930e-03, 7.1398e-01, 1.0104e+00, 1.0242e+00, 6.8070e-01,\n",
            "        9.9616e-01, 5.0565e-01, 4.1435e-01, 3.0730e-01, 3.4557e-01, 1.3315e+00,\n",
            "        8.0923e-04, 1.0445e+00, 6.0952e-04, 9.9565e-02, 7.0488e-01, 7.0627e-04,\n",
            "        1.7248e+00, 3.9991e-01, 2.0378e-01, 1.7900e+00, 7.4258e-01, 5.9122e-01,\n",
            "        1.4910e-01, 5.9228e-01, 1.1001e-01, 4.1941e-01], device='cuda:0')\n",
            "rank [32 57 54 28  0 11 47 29 17 23 49 40 39 42 30  1  5 58 38 52 21 41 36  9\n",
            " 61 59 24 31 25  2 43 63 44 55  7  6 46  3 27 10 45 26 18 15 20  4 56 16\n",
            " 34 60 14 22 62 19 51 12  8 37 48 33 53 50 13 35]\n",
            "Processing layer 2, original layer has 64 filters, pruning model has 48 filters\n",
            "torch.Size([64, 64, 3, 3])\n",
            "tensor([0.9568, 0.7575, 1.0117, 0.6820, 0.7745, 1.0443, 0.8286, 0.7457, 1.3126,\n",
            "        0.9962, 1.0340, 0.7761, 0.9243, 1.2169, 0.9924, 1.0401, 0.9995, 0.9957,\n",
            "        0.9023, 0.8100, 0.8217, 0.9245, 0.8151, 0.9718, 1.0651, 1.1251, 0.7104,\n",
            "        0.9538, 0.8629, 1.0188, 0.9096, 1.1071, 0.8531, 0.9000, 1.0658, 0.6484,\n",
            "        1.0872, 1.3863, 1.0284, 0.8155, 0.8064, 0.9070, 0.7479, 0.8785, 1.0194,\n",
            "        1.0796, 0.6774, 0.8222, 0.7790, 0.9399, 0.8873, 1.0353, 1.2583, 0.7162,\n",
            "        1.0076, 0.8251, 0.8711, 0.8425, 1.1224, 1.0227, 1.1827, 0.7698, 0.7703,\n",
            "        1.0230], device='cuda:0')\n",
            "rank [37  8 52 13 60 25 58 31 36 45 34 24  5 15 51 10 38 63 59 44 29  2 54 16\n",
            "  9 17 14 23  0 27 49 21 12 30 41 18 33 50 43 56 28 32 57  6 55 47 20 39\n",
            " 22 19 40 48 11  4 62 61  1 42  7 53 26  3 46 35]\n",
            "Processing layer 3, original layer has 128 filters, pruning model has 96 filters\n",
            "torch.Size([128, 64, 3, 3])\n",
            "tensor([0.9056, 1.2032, 0.8065, 1.0508, 0.6540, 0.8431, 0.8092, 1.1048, 0.6802,\n",
            "        0.6213, 0.7663, 1.0939, 0.6523, 0.8082, 1.0406, 1.1564, 0.7518, 0.8792,\n",
            "        0.8538, 0.6408, 0.9373, 0.9123, 0.5689, 0.9607, 0.7212, 0.8859, 0.7632,\n",
            "        1.0145, 0.9863, 1.0127, 0.7843, 0.8412, 0.9535, 0.6994, 0.8776, 0.9314,\n",
            "        0.8150, 0.9517, 1.0069, 0.7411, 1.0646, 0.6929, 0.9094, 0.9399, 0.8157,\n",
            "        1.0090, 1.0515, 0.7427, 0.9932, 0.7515, 0.9520, 0.8851, 0.8249, 0.8667,\n",
            "        0.9560, 0.8266, 1.2360, 0.8769, 0.8644, 0.7465, 1.0445, 0.6287, 0.9047,\n",
            "        0.9438, 0.9550, 0.8328, 0.8971, 0.8171, 0.7848, 0.7398, 1.1947, 0.7917,\n",
            "        0.7963, 0.9090, 0.8877, 0.8802, 1.0609, 0.8857, 0.9349, 0.7314, 0.9118,\n",
            "        0.8175, 0.8522, 0.9819, 1.1633, 0.9387, 0.9145, 0.8825, 0.9989, 0.9490,\n",
            "        0.9121, 0.7317, 1.0114, 0.7144, 0.5084, 0.9593, 0.7101, 0.9798, 0.8486,\n",
            "        1.0480, 0.8610, 0.7708, 1.0563, 0.5679, 1.0435, 0.8778, 0.8651, 0.8893,\n",
            "        0.6839, 0.8352, 0.9592, 1.1133, 0.8513, 0.9218, 0.7519, 0.7551, 0.6435,\n",
            "        1.0919, 0.8421, 0.8340, 0.9382, 0.9341, 0.7823, 0.9830, 0.8949, 0.7779,\n",
            "        0.7818, 0.8259], device='cuda:0')\n",
            "rank [ 56   1  70  84  15 111   7  11 117  40  76 102  46   3  99  60 104  14\n",
            "  27  29  92  45  38  88  48  28 123  83  97  23  95 110  54  64  32  50\n",
            "  37  89  63  43  85 120  20  78 121  35 113  86  21  90  80  42  73   0\n",
            "  62  66 124 107  74  25  77  51  87  75  17 105  34  57  53 106  58 100\n",
            "  18  82 112  98   5 118  31 109 119  65  55 127  52  81  67  44  36   6\n",
            "  13   2  72  71  68  30 122 126 125 101  10  26 115 114  16  49  59  47\n",
            "  39  69  91  79  24  93  96  33  41 108   8   4  12 116  19  61   9  22\n",
            " 103  94]\n",
            "Processing layer 4, original layer has 128 filters, pruning model has 96 filters\n",
            "torch.Size([128, 128, 3, 3])\n",
            "tensor([1.0231, 1.0454, 1.2206, 1.0917, 1.2518, 1.1505, 1.1474, 1.0384, 1.0518,\n",
            "        0.9048, 0.9241, 1.0116, 1.0843, 1.2857, 1.1812, 1.2578, 1.1087, 1.1351,\n",
            "        0.9463, 1.0858, 0.9617, 0.9937, 0.9517, 0.8922, 1.1173, 1.1034, 1.0607,\n",
            "        1.1873, 1.0445, 1.0623, 1.1709, 1.0459, 0.9131, 1.0693, 0.9923, 1.0882,\n",
            "        1.0070, 1.1053, 1.1287, 1.1162, 1.1155, 1.0448, 0.9768, 0.9106, 1.1081,\n",
            "        1.2872, 1.0263, 0.9726, 1.1461, 1.0607, 1.0892, 1.1698, 0.9422, 1.0944,\n",
            "        1.1779, 0.9242, 0.8192, 0.9849, 1.0097, 1.0721, 0.9904, 1.0608, 1.0419,\n",
            "        0.7969, 1.0923, 0.9972, 1.1887, 0.9242, 1.0014, 0.9234, 0.8947, 1.0463,\n",
            "        1.1040, 1.1585, 1.0215, 1.1115, 1.1072, 0.9435, 1.0690, 0.9920, 1.1128,\n",
            "        0.9135, 0.9809, 1.0261, 0.7960, 1.1044, 1.0139, 0.8884, 1.1172, 1.0516,\n",
            "        0.9758, 1.0344, 1.0033, 1.0105, 1.0380, 1.1240, 1.0154, 1.0536, 0.9179,\n",
            "        1.1002, 1.0663, 1.0195, 0.9894, 1.0369, 1.0963, 0.9281, 1.1952, 1.2703,\n",
            "        1.0427, 0.9608, 1.2018, 0.9992, 0.9976, 1.0670, 1.0453, 0.9500, 1.0411,\n",
            "        1.1848, 0.9663, 1.0476, 1.0834, 1.2515, 1.2724, 1.0816, 1.0633, 0.9146,\n",
            "        1.2131, 1.0363], device='cuda:0')\n",
            "rank [ 45  13 122 107  15   4 121   2 126 110 106  66  27 117  14  54  30  51\n",
            "  73   5   6  48  17  38  95  24  88  39  40  80  75  16  44  76  37  85\n",
            "  72  25  99 104  53  64   3  50  35  19  12 120 123  59  33  78 113 100\n",
            " 124  29  61  49  26  97   8  89 119  71  31   1 114  41  28 108  62 116\n",
            "   7  94 103 127  91  46  83   0  74 101  96  86  11  93  58  36  92  68\n",
            " 111 112  65  21  34  79  60 102  57  82  42  90  47 118  20 109  22 115\n",
            "  18  77  52 105  55  67  10  69  98 125  81  32  43   9  70  23  87  56\n",
            "  63  84]\n",
            "Processing layer 5, original layer has 256 filters, pruning model has 192 filters\n",
            "torch.Size([256, 128, 3, 3])\n",
            "tensor([0.8461, 0.9232, 0.9796, 1.1330, 0.9846, 0.9890, 0.9906, 0.8667, 0.8769,\n",
            "        1.0311, 1.0158, 0.7897, 0.8149, 0.9340, 1.0628, 1.0023, 1.0149, 0.9040,\n",
            "        0.8680, 0.9649, 0.9560, 0.8570, 0.9436, 0.7777, 0.8655, 0.8244, 1.0308,\n",
            "        0.8641, 0.9364, 0.9781, 0.8858, 0.7877, 0.9450, 1.0429, 0.9415, 1.0126,\n",
            "        0.9076, 1.0077, 0.9098, 0.8726, 0.9587, 1.0014, 0.9070, 0.7537, 1.0207,\n",
            "        1.0161, 0.9512, 0.7911, 0.8965, 0.9276, 0.8665, 0.8889, 0.8957, 0.9366,\n",
            "        1.0546, 0.7356, 1.0615, 0.8487, 0.7670, 0.8657, 1.0829, 0.8350, 0.9501,\n",
            "        0.8192, 0.9940, 1.0186, 1.0370, 0.9247, 0.8490, 0.9787, 0.8965, 0.8848,\n",
            "        0.8765, 0.9511, 0.9411, 0.9688, 0.8671, 0.9115, 0.9581, 0.9502, 0.9638,\n",
            "        0.9066, 0.8960, 0.9372, 0.9483, 0.9139, 1.0482, 1.0611, 0.9499, 0.7736,\n",
            "        0.9059, 0.9403, 0.9167, 1.0857, 0.8858, 0.9932, 0.8763, 0.8811, 0.9114,\n",
            "        0.8832, 0.9335, 1.1627, 0.8005, 0.9833, 0.8880, 0.9150, 0.9251, 0.9277,\n",
            "        0.7593, 0.9075, 0.8598, 0.9795, 1.0132, 0.8847, 0.7912, 0.9590, 0.8568,\n",
            "        1.1233, 0.9601, 0.9357, 0.8982, 0.9408, 0.8780, 0.9030, 0.9698, 1.0651,\n",
            "        1.0717, 0.9525, 0.9857, 0.9649, 0.9884, 0.9605, 0.9853, 1.0957, 0.8976,\n",
            "        0.8067, 0.9314, 1.1078, 0.9898, 0.8442, 1.0309, 0.9896, 1.0526, 0.9674,\n",
            "        0.7821, 0.8983, 0.9930, 1.0320, 0.8825, 0.9546, 0.9959, 0.9414, 0.8891,\n",
            "        1.0347, 1.0175, 0.8817, 0.9771, 0.8960, 0.9602, 0.9515, 0.8791, 0.9918,\n",
            "        1.0068, 0.9165, 0.9175, 0.9338, 0.9524, 0.8509, 0.8345, 1.0035, 0.8920,\n",
            "        1.0554, 0.9472, 0.8728, 0.8593, 0.9629, 0.8994, 0.8529, 0.9275, 0.9571,\n",
            "        0.8996, 0.9324, 0.9050, 0.8007, 0.8998, 0.9872, 0.9896, 0.8734, 0.9505,\n",
            "        0.9816, 0.9393, 1.0159, 0.9099, 0.9813, 0.8866, 0.8663, 0.9328, 0.9581,\n",
            "        1.0526, 0.9851, 0.9504, 0.8333, 0.9199, 0.8959, 0.9159, 0.8936, 0.9308,\n",
            "        0.8811, 0.9378, 0.9415, 1.0588, 0.9794, 0.9934, 0.8327, 0.7932, 0.9004,\n",
            "        0.8929, 0.9057, 1.1133, 0.8710, 1.0030, 1.0492, 1.0250, 1.0269, 0.8406,\n",
            "        0.9501, 1.0673, 0.9295, 0.9956, 1.0263, 0.9736, 0.8417, 0.8730, 0.8783,\n",
            "        0.8618, 1.0596, 0.8860, 1.0788, 1.0146, 0.8570, 0.9566, 0.9563, 0.9213,\n",
            "        0.8256, 0.8886, 0.8906, 0.9696, 0.8690, 0.7900, 0.9356, 0.9320, 0.9529,\n",
            "        0.8635, 0.8925, 0.6859, 0.9850], device='cuda:0')\n",
            "rank [101   3 117 218 137 133  93  60 237 126 226 125  14  56  87 235 210 171\n",
            "  54 198 142 221  86  33  66 153 147   9 140  26 223 229 222  44  65 154\n",
            "  45 191  10  16 238 112  35  37 162 169 220  15  41 150 228  64 212  95\n",
            " 146 161   6 138 141 186   5 130 185 128 132 199 255   4 103 189 193   2\n",
            " 111 211  69  29 156 230 124 246  75 143 129  19  80 175 131 158 118 115\n",
            "  40  78 197 179 240 241  20 149 251 127 166 159  46  73 188 200  79 225\n",
            "  62  88  84 172  32  22  34 209 151  74 121  91 190 208  83  53  28 119\n",
            " 249  13 165 100 196 181 250 136 206 227 107  49 178 106  67   1 242 202\n",
            " 164  92 163 204 105  85  77  98 192  38  36 109  42  81  90 217 182  17\n",
            " 123 215 184 180 176 145 120 134  70  48  82 157 203  52 205 216 253 170\n",
            " 245 152  51 244 104 194 236  94  30  71 113  99 148 155  97 207 160 233\n",
            " 122   8  72  96 187 232 173  39 219 247  18  76   7  50 195  59  24  27\n",
            " 252 234 110 174  21 239 116 177 167  68  57   0 139 231 224  61 168 201\n",
            " 213 243  25  63  12 135 183 102 214 114  47 248  11  31 144  23  89  58\n",
            " 108  43  55 254]\n",
            "Processing layer 6, original layer has 256 filters, pruning model has 192 filters\n",
            "torch.Size([256, 256, 3, 3])\n",
            "tensor([1.1393, 0.9696, 0.9510, 0.9116, 1.0520, 0.8667, 0.9868, 1.1479, 0.7644,\n",
            "        1.0532, 1.1246, 1.0023, 1.0956, 1.0918, 0.9615, 0.8127, 1.0691, 1.0374,\n",
            "        0.9313, 1.1300, 0.8395, 0.8365, 1.0507, 0.9030, 0.9064, 1.0578, 0.7801,\n",
            "        1.0096, 0.8538, 1.1821, 1.0081, 1.0181, 0.9270, 1.0705, 0.9107, 0.9102,\n",
            "        1.0551, 1.1923, 0.8286, 0.8251, 0.9371, 1.0202, 0.9889, 0.9843, 0.9155,\n",
            "        1.1469, 1.1198, 0.8400, 1.0151, 0.8569, 1.0574, 1.1069, 0.9810, 0.9440,\n",
            "        0.8464, 1.0430, 1.0848, 1.3528, 1.1154, 1.1277, 1.1040, 0.9503, 1.1370,\n",
            "        1.0350, 0.8754, 1.0116, 0.7670, 0.6487, 0.9960, 0.9508, 0.9538, 0.8562,\n",
            "        0.8853, 0.8682, 0.8843, 1.0566, 0.9573, 0.9023, 1.0475, 0.8780, 1.0285,\n",
            "        0.9444, 0.9884, 1.0163, 0.9612, 0.8453, 1.0123, 0.8693, 0.9603, 1.0276,\n",
            "        0.9697, 1.0618, 1.0649, 1.0517, 0.8783, 1.0818, 0.9609, 0.8736, 0.9446,\n",
            "        0.9675, 0.9834, 0.9508, 0.9670, 0.9143, 1.0972, 1.0355, 0.9813, 0.8556,\n",
            "        0.9035, 0.8944, 1.0551, 1.1224, 0.9203, 1.0111, 0.8077, 0.9693, 1.0669,\n",
            "        0.9601, 0.9472, 1.0454, 0.9045, 1.1342, 1.1666, 0.9442, 1.0338, 0.8765,\n",
            "        0.9417, 1.0989, 0.9517, 0.8746, 0.8425, 0.9688, 0.9338, 1.0517, 1.0503,\n",
            "        1.0142, 0.9762, 1.1257, 0.8447, 1.0155, 1.1394, 1.0343, 0.8689, 1.0478,\n",
            "        1.1028, 0.8719, 0.7800, 0.9544, 0.9602, 1.1345, 1.0896, 0.9747, 0.9461,\n",
            "        1.0065, 1.2760, 0.9591, 0.9925, 0.8729, 0.9115, 1.0381, 0.9585, 0.9471,\n",
            "        0.9919, 0.9065, 1.1755, 1.0534, 0.9864, 1.0049, 1.0526, 0.8026, 0.9390,\n",
            "        1.0453, 0.9771, 1.1384, 0.9309, 1.0480, 0.9945, 1.0393, 0.9875, 1.0261,\n",
            "        0.8767, 1.0289, 0.8802, 0.9386, 1.0452, 1.0041, 0.7416, 1.0413, 1.0704,\n",
            "        0.9601, 0.9465, 0.9273, 0.9905, 0.8516, 1.0013, 0.8359, 0.9831, 1.0658,\n",
            "        1.0772, 1.0348, 0.8670, 1.0601, 0.9204, 1.1269, 0.9374, 0.9424, 0.8664,\n",
            "        0.9996, 1.0622, 1.2281, 0.8689, 0.9494, 0.9564, 1.0885, 0.9561, 0.8894,\n",
            "        0.9992, 1.0544, 0.9667, 1.0579, 1.0753, 0.8832, 1.0229, 1.1603, 1.0246,\n",
            "        0.9265, 0.9192, 0.9569, 0.8528, 0.9310, 0.9918, 0.8742, 0.9164, 0.8442,\n",
            "        1.0951, 0.8695, 1.0501, 1.0618, 1.0269, 1.0077, 1.0783, 1.0561, 0.9661,\n",
            "        1.0579, 0.7767, 1.0725, 0.9627, 1.0964, 0.9178, 0.9932, 0.7609, 0.9281,\n",
            "        0.9790, 0.8548, 0.8784, 0.9103], device='cuda:0')\n",
            "rank [ 57 154 209  37  29 164 122 223   7  45 140   0 173  62 149 121  19  59\n",
            " 203 137  10 111  46  58  51  60 144 127 104 247  12 234  13 150 213  56\n",
            "  95 240 198 220 245  33 188  16 116 197  92 208  91 237 201 243 219  25\n",
            "  50  75 241  36 110 217 165   9 168   4 133  93  22 134 236 175 143  78\n",
            " 119 171 184  55 187 177 159  17 105  63 199 141 124 181  80  89 238 179\n",
            " 224 222  41  31  83 139  48 135  86  65 113  27  30 239 153 167 185  11\n",
            " 194 207 216  68 176 249 156 162 230 192  42  82 178   6 166  43 100 196\n",
            " 106  52 252 172 136 151  90   1 115 131  99 102 218 242 246  14  84  96\n",
            "  88 148 117 189 155 160  76 227 212 214 147  70 128   2  69 101  61 211\n",
            " 118 161 190 152  98  81 123  53 205 126 170 183 204  40 132  18 229 174\n",
            " 251 191  32 225 202 112 226 248 232  44 103   3 158  34 255  35 163  24\n",
            " 120 108  23  77 109 215  72  74 221 182 254  94  79 180 125  64 129 231\n",
            "  97 157 145 235  87 142 210  73 200   5 206  49  71 107 253  28 228 193\n",
            "  54  85 138 233 130  47  20  21 195  38  39  15 114 169  26 146 244  66\n",
            "   8 250 186  67]\n",
            "Processing layer 7, original layer has 256 filters, pruning model has 192 filters\n",
            "torch.Size([256, 256, 3, 3])\n",
            "tensor([0.7839, 0.8768, 0.8562, 0.9486, 0.9061, 0.7997, 0.8143, 0.8145, 0.9756,\n",
            "        1.0431, 0.9903, 0.8046, 0.7741, 0.9971, 0.8233, 0.9227, 0.9413, 1.0159,\n",
            "        0.9788, 0.9140, 0.8190, 0.8595, 0.7210, 0.8175, 0.8243, 0.9547, 0.8243,\n",
            "        1.0325, 0.8497, 1.0122, 0.8960, 0.9452, 0.9197, 0.8480, 1.0316, 0.7774,\n",
            "        0.9913, 0.8952, 0.9929, 0.9208, 0.9992, 0.7717, 0.8859, 0.8514, 0.8318,\n",
            "        0.9066, 0.9071, 0.8845, 0.7249, 0.7035, 0.8327, 0.6884, 0.8355, 0.6832,\n",
            "        0.9035, 0.8975, 0.8683, 0.7731, 0.9233, 0.8603, 1.0390, 0.9802, 0.9308,\n",
            "        0.7633, 1.0618, 0.7983, 0.9149, 0.7910, 0.8376, 1.0066, 0.9383, 0.9532,\n",
            "        1.0268, 0.8929, 0.8017, 0.9182, 0.7442, 1.0297, 0.9136, 0.8814, 0.8931,\n",
            "        0.8609, 0.9256, 0.8824, 0.8213, 0.9019, 0.8187, 0.9203, 0.9524, 1.0831,\n",
            "        0.7759, 0.9059, 0.7137, 0.9950, 0.9618, 0.7506, 0.9667, 0.9181, 0.8219,\n",
            "        0.6471, 0.8669, 0.8859, 0.8240, 0.8752, 0.7824, 0.9758, 0.9872, 0.8351,\n",
            "        0.9336, 0.9379, 0.8962, 1.1295, 0.9657, 0.9086, 0.8770, 0.7431, 0.8882,\n",
            "        0.8920, 0.8541, 0.9433, 0.8165, 1.0099, 0.8478, 0.9050, 0.9038, 0.9046,\n",
            "        0.8334, 1.0541, 1.0066, 0.7870, 1.0096, 0.9163, 0.9658, 1.0967, 0.8316,\n",
            "        0.7749, 0.7828, 0.8531, 0.8589, 0.9624, 0.8473, 0.8773, 0.7472, 0.9231,\n",
            "        0.8641, 0.9830, 0.8581, 0.8695, 1.0000, 0.7478, 0.9378, 0.7647, 0.8377,\n",
            "        0.8599, 0.8179, 0.9144, 0.9026, 0.8772, 0.8479, 0.9365, 0.7205, 1.0061,\n",
            "        0.9447, 0.8891, 0.8639, 0.9314, 0.9675, 0.9481, 0.6345, 0.9697, 0.8793,\n",
            "        0.9940, 0.8485, 1.0168, 0.9258, 0.9469, 0.9591, 0.9906, 1.0447, 0.7510,\n",
            "        0.9471, 0.9715, 0.9519, 0.9582, 0.9950, 0.8914, 0.8584, 0.9073, 0.8666,\n",
            "        0.8748, 0.8694, 0.8852, 0.8446, 0.8069, 0.7229, 0.7869, 0.9028, 0.8037,\n",
            "        0.8726, 0.8735, 0.8469, 0.9880, 0.9709, 0.8820, 0.8146, 0.8738, 0.7977,\n",
            "        0.9326, 0.9061, 0.8691, 0.8462, 0.7919, 1.0603, 0.9905, 0.9209, 0.8138,\n",
            "        1.0399, 0.8857, 1.0841, 1.0968, 0.9400, 0.8941, 0.9675, 0.7573, 0.9557,\n",
            "        0.8363, 0.7843, 0.8476, 0.8529, 0.8605, 0.8286, 1.0321, 0.8462, 1.0932,\n",
            "        1.0173, 0.9147, 0.8533, 1.0235, 1.0716, 0.8273, 0.8493, 0.9830, 0.9971,\n",
            "        0.9383, 0.9026, 0.8821, 0.9392, 0.9522, 0.8409, 0.9194, 0.7814, 0.9909,\n",
            "        0.8878, 0.9710, 0.8551, 0.7729], device='cuda:0')\n",
            "rank [111 219 133 233 218  89 238  64 212 127 178   9 216  60  27 231  34  77\n",
            "  72 237 234 173  17  29 121 130 128  69 161 148  40  13 242  93 184 171\n",
            "  38  36 251 177 213  10 201 106 241 145  61  18 105   8 181 253 202 169\n",
            " 222 166  96 132 112 139  94 176 183 224  25  71  88 247 182   3 167 180\n",
            " 175  31 162 119  16 220 246 243  70 109 150 159 108 207 165  62 174  82\n",
            "  58 143  15 214  39  87  32 249  75  97 131  66 235 155  19  78 113 187\n",
            "  46  45   4 208  91 123 125 124  54 196 244 156  85  55 110  30  37 221\n",
            "  80  73 117 185 163 116 252 101  42 217 191  47  83 245 203  79 170 141\n",
            " 157 114   1 103 189 205 199 198 147 190 209  56 100 188 144 164  81 229\n",
            "  59 153  21 138 186 146   2 254 118 236 137 228  43  28 240 172  33 158\n",
            " 122 227 140 200 232 210 192 248 152  68 225  52 107 126  50  44 134 230\n",
            " 239  24  26 102  14  98  84  20  86 154  23 120 204   7   6 215 193  11\n",
            " 197  74   5  65 206 211  67 129 195 226   0 136 104 250  35  90 135  12\n",
            "  57 255  41 151  63 223 179  95 149 142  76 115  48 194  22 160  92  49\n",
            "  51  53  99 168]\n",
            "Processing layer 8, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 256, 3, 3])\n",
            "tensor([0.3924, 0.6133, 0.3899, 0.9467, 0.4961, 0.5762, 0.5570, 0.6759, 0.7545,\n",
            "        0.4806, 0.3990, 0.6177, 0.4761, 0.6208, 0.4477, 0.5866, 0.8410, 0.4024,\n",
            "        0.4413, 0.4484, 0.4107, 0.4374, 1.1125, 0.4102, 0.6466, 0.4517, 0.4721,\n",
            "        0.5022, 0.5002, 0.5039, 0.5700, 0.5343, 0.5459, 0.5483, 0.6681, 0.6981,\n",
            "        0.6710, 0.3195, 0.6760, 0.4015, 0.4123, 0.5274, 0.3886, 0.4656, 0.5218,\n",
            "        0.2372, 0.3832, 0.5567, 0.5187, 0.5661, 0.7076, 0.4699, 0.6495, 0.3480,\n",
            "        0.3431, 0.4385, 0.2167, 0.3901, 0.5099, 0.5983, 0.3871, 0.4936, 0.6564,\n",
            "        0.6305, 0.4258, 0.7851, 0.5828, 0.3116, 0.4569, 0.3722, 0.5134, 0.4672,\n",
            "        0.4845, 0.5122, 0.5736, 0.5456, 0.5067, 0.4668, 0.4579, 0.5934, 0.3571,\n",
            "        0.4708, 0.5206, 0.4440, 0.6514, 0.3599, 0.4914, 0.7972, 0.4163, 0.4504,\n",
            "        0.6568, 0.3539, 0.5716, 0.4440, 0.4010, 0.5316, 0.6618, 0.4469, 0.5581,\n",
            "        0.3736, 0.4793, 0.4766, 0.6713, 0.5557, 0.3356, 0.3927, 0.3674, 0.4242,\n",
            "        0.7505, 0.5591, 0.5040, 0.4323, 0.7897, 0.5425, 0.5993, 0.4331, 0.4932,\n",
            "        0.3885, 0.6136, 0.3901, 0.3425, 0.6444, 0.4432, 0.4673, 0.6620, 0.3593,\n",
            "        0.4477, 0.4857, 0.3903, 0.6651, 0.4373, 0.5386, 0.5597, 0.5889, 0.5413,\n",
            "        0.4326, 0.3421, 0.4676, 0.4335, 0.5567, 0.5553, 0.6559, 0.5152, 0.4001,\n",
            "        0.3121, 0.4356, 0.6130, 0.6450, 0.5306, 0.3481, 0.5224, 0.4614, 0.5384,\n",
            "        0.6991, 0.5658, 0.5401, 0.4712, 0.4820, 0.4003, 0.3808, 0.4720, 0.6524,\n",
            "        0.4218, 0.5804, 0.4490, 0.4951, 0.5194, 0.6838, 0.4814, 0.7650, 0.6185,\n",
            "        0.4836, 0.5801, 0.6259, 0.4990, 0.8079, 0.4546, 0.3882, 0.6490, 0.5178,\n",
            "        0.6226, 0.4189, 0.4247, 0.4634, 0.5088, 0.3730, 0.7114, 0.3933, 0.4754,\n",
            "        0.7543, 0.4116, 0.4676, 0.4522, 0.5826, 0.6575, 0.4700, 0.5384, 0.5927,\n",
            "        0.4495, 0.6460, 0.4964, 0.9203, 0.6935, 0.2664, 0.5852, 0.7320, 0.5053,\n",
            "        0.5384, 0.4537, 0.4157, 0.4218, 0.3371, 0.6926, 0.7835, 0.5297, 0.4602,\n",
            "        0.3605, 0.5295, 0.4991, 0.4116, 0.4285, 0.4131, 0.3683, 0.7295, 0.6597,\n",
            "        0.3345, 0.4370, 0.5828, 0.6093, 0.5777, 0.6341, 0.5452, 0.4250, 0.3497,\n",
            "        0.5684, 0.7408, 0.3753, 0.5261, 0.5595, 0.4400, 0.4328, 0.4098, 0.5739,\n",
            "        0.7755, 0.4901, 0.5687, 0.4942, 0.4371, 0.5660, 0.5116, 0.5864, 0.5540,\n",
            "        0.6307, 0.3327, 0.6252, 0.3648, 0.4792, 0.5011, 0.6262, 0.6844, 0.5583,\n",
            "        0.5370, 0.4653, 0.5789, 0.6148, 0.5938, 0.4343, 0.6003, 0.3795, 0.3240,\n",
            "        0.4162, 0.7324, 0.2632, 0.8069, 0.5971, 0.6302, 0.4618, 0.4443, 0.6695,\n",
            "        0.5260, 0.6282, 0.3692, 0.7309, 0.4783, 0.4417, 0.6401, 0.5716, 0.3153,\n",
            "        0.3465, 0.4300, 0.7412, 0.7173, 0.6073, 0.4054, 0.6345, 0.5662, 0.3262,\n",
            "        0.5073, 0.2813, 0.7256, 0.5212, 0.5372, 0.5561, 0.5409, 0.6336, 0.4586,\n",
            "        0.6366, 0.5033, 0.5094, 0.6218, 0.5177, 0.6444, 0.4122, 0.4210, 0.7903,\n",
            "        0.7012, 0.5341, 0.4039, 0.4175, 0.5128, 0.5025, 0.7674, 0.5655, 0.5434,\n",
            "        0.5908, 0.5800, 0.4712, 0.3333, 0.2910, 0.4928, 0.3398, 0.5611, 0.5851,\n",
            "        0.4421, 0.5179, 0.4087, 0.5445, 0.6571, 0.3538, 0.5895, 0.5962, 0.5476,\n",
            "        0.6136, 0.7130, 0.3622, 0.4251, 0.4558, 0.5062, 0.4828, 0.3380, 0.6538,\n",
            "        0.3836, 0.6527, 0.6342, 0.4643, 0.6125, 0.5419, 0.5220, 0.4694, 0.6890,\n",
            "        0.6008, 0.5018, 0.5797, 0.3552, 0.5235, 0.4839, 0.4321, 0.2481, 0.6313,\n",
            "        0.5043, 0.5482, 0.5052, 0.5337, 0.4572, 0.5084, 0.2658, 0.5275, 0.5335,\n",
            "        0.4931, 0.4757, 0.4492, 0.4125, 0.5065, 0.4437, 0.5724, 0.6154, 0.5792,\n",
            "        0.3372, 0.5136, 0.6033, 0.2804, 0.5933, 0.5751, 0.5293, 0.4818, 0.6502,\n",
            "        0.3424, 0.5428, 0.5031, 0.3929, 0.3607, 0.4360, 0.6102, 0.5603, 0.7173,\n",
            "        0.5240, 0.6039, 0.6245, 0.4757, 0.4523, 0.5882, 0.4410, 0.3977, 0.7261,\n",
            "        0.5695, 0.4955, 0.5977, 0.4178, 0.5565, 0.3571, 0.4837, 0.3441, 0.3009,\n",
            "        0.4668, 0.3464, 0.3586, 0.5385, 0.7875, 0.6152, 0.4557, 0.7054, 0.4251,\n",
            "        0.5406, 0.3099, 0.4323, 0.4293, 0.3430, 0.3205, 0.6419, 0.5169, 0.6118,\n",
            "        0.5561, 0.3918, 0.5652, 0.5192, 0.4280, 0.4678, 0.5639, 0.5742, 0.5660,\n",
            "        0.4750, 0.3989, 0.3431, 0.5194, 0.7609, 0.3704, 0.6986, 0.4020, 0.2842,\n",
            "        0.5286, 0.5041, 0.6232, 0.4270, 0.5367, 0.6719, 0.4966, 0.7415, 0.3840,\n",
            "        0.5466, 0.3884, 0.4882, 0.5128, 0.4682, 0.5324, 0.5345, 0.6196, 0.3551,\n",
            "        0.5294, 0.4088, 0.5796, 0.7244, 0.4406, 0.5457, 0.5045, 0.6027, 0.6109,\n",
            "        0.5069, 0.3619, 0.5287, 0.8189, 0.6256, 0.6059, 0.5192, 0.6831, 0.3190,\n",
            "        0.8204, 0.4033, 0.5597, 0.4359, 0.4886, 0.5763, 0.5572, 0.6171, 0.5528,\n",
            "        0.6936, 0.2698, 0.4502, 0.4139, 0.4815, 0.5012, 0.5588, 0.5208],\n",
            "       device='cuda:0')\n",
            "rank [ 22   3 201  16 495 489 175 273  87 314 112 427  65 213 243 321 169 454\n",
            "   8 189 108 466 290 235 271 205 282 223 413 299 480 291 404 343 186  50\n",
            " 430 315 153 456  35 504 202 212 359 259 167 493  38   7 464 102  36 278\n",
            "  34 129 124  96 224 194 337  90  62 141 350 352 161  84 395  52 178  24\n",
            " 199 147 311 121 438 285 306 294 353 230 304 368 252  63 275 280 258 173\n",
            " 490 254 407 461 180 309  13 475 170  11 502 385 428 264 342 118   1 146\n",
            " 355 440 485 402 228 292 491 406 389 484 360 267 114  59 416 274 340 265\n",
            "  79 391 197 324 339 133 410  15 250 204 332 227  66 193 163 172 325 362\n",
            " 479 386 263 229 500   5 392 448 242  74 384  92 286  30 414 245 234 295\n",
            "  49 248 449 154 322 443 447 331 403 132 497 238 109 510 260  98 501   6\n",
            " 139  47 418 302 441 103 140 251 503  33 370 341 468  32 482  75 231 336\n",
            " 323 397 113 356 134 303 432 155 131 426 207 152 196 301 261 463 474  31\n",
            " 316 372 377 473  95 148 214 217 477 393 488 459 376  41 237 279 405 364\n",
            " 150 357  44 300 511  82 166 453 492 444  48 334 179 310 439 142 388  70\n",
            " 319 471  73 249  58 308 184 374 297 486  76 382 347 206 371 483 369 460\n",
            " 110  29 307 398 320  27 361 509 257  28 218 174 465 200   4 415 165 246\n",
            "  61 116 378 329  86 244 499 470 127  72 365 420 171 348 157 394 508 168\n",
            "   9 100 256 283 101  12 408 379 188 450  26 160 326 156  81 195  51 358\n",
            " 472 446 137 191 123  71 423  77  43 262 354 183 276 151 215 305  78 373\n",
            "  68 346 429 176 208 409 192  25  89 506 198 380 164  19  14 126  97 277\n",
            "  93  83 383 122 333 284  18 411 481 239  55  21 130 247 226 401 498 145\n",
            " 266 138 115 240 135 434 111 366 289 435 220 445 462  64 431 345 232 182\n",
            " 107 162 210 313 181 417 318  88 270 209 507 221 381  40 312 219 190  20\n",
            "  23 241 478 335 293 317 496  17 457  39  94 158 143  10 451 412 187 399\n",
            " 105   0 442 128  57 119   2  42 117 469 177  60 467 351  46 159 268 236\n",
            "  99 185  69 455 281 222 106 255 344 487 400 216  85 125 425 419  80 363\n",
            " 476  91 338 233 149  53 288 424 421  54 452 436 120 396 136 330 349 387\n",
            " 211 104 225 327 253 296 269 437  37 494 287 144  67 433 422 328 458 298\n",
            " 390 505 203 375 272 367  45  56]\n",
            "Processing layer 9, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.3400, 0.3731, 0.2459, 0.2907, 0.5719, 0.1646, 0.2407, 0.3343, 0.2825,\n",
            "        0.4461, 0.4344, 0.4081, 0.2097, 0.3693, 0.3085, 0.1512, 0.2547, 0.4675,\n",
            "        0.2605, 0.4644, 0.4318, 0.4732, 0.3245, 0.4159, 0.2407, 0.2925, 0.1827,\n",
            "        0.2949, 0.2251, 0.3583, 0.3995, 0.4264, 0.3285, 0.2499, 0.2370, 0.2478,\n",
            "        0.2691, 0.3596, 0.3553, 0.3680, 0.2456, 0.2967, 0.3415, 0.2241, 0.2506,\n",
            "        0.2622, 0.3264, 0.3920, 0.2327, 0.2689, 0.2618, 0.4584, 0.3417, 0.3433,\n",
            "        0.3565, 0.3050, 0.2296, 0.2493, 0.2967, 0.2265, 0.5685, 0.1965, 0.2792,\n",
            "        0.3499, 0.3358, 0.3238, 0.3665, 0.3009, 0.2390, 0.2480, 0.3193, 0.3732,\n",
            "        0.2667, 0.4316, 0.2145, 0.2115, 0.4031, 0.2972, 0.2987, 0.4437, 0.3188,\n",
            "        0.2943, 0.2752, 0.2920, 0.2784, 0.2351, 0.3840, 0.4324, 0.3354, 0.3488,\n",
            "        0.3367, 0.4669, 0.2559, 0.3480, 0.2603, 0.2919, 0.2094, 0.2071, 0.2595,\n",
            "        0.3414, 0.2220, 0.3379, 0.2774, 0.3659, 0.2882, 0.4035, 0.2776, 0.3435,\n",
            "        0.2911, 0.1778, 0.4086, 0.2823, 0.3428, 0.2871, 0.2928, 0.3820, 0.3467,\n",
            "        0.3262, 0.2657, 0.3130, 0.1607, 0.3842, 0.3451, 0.2667, 0.4301, 0.2760,\n",
            "        0.1702, 0.3240, 0.4059, 0.3450, 0.2451, 0.2912, 0.2979, 0.4333, 0.3159,\n",
            "        0.3132, 0.1858, 0.4955, 0.2672, 0.1867, 0.2695, 0.3737, 0.3523, 0.4612,\n",
            "        0.2010, 0.2559, 0.2461, 0.4458, 0.4148, 0.4862, 0.3349, 0.5248, 0.1756,\n",
            "        0.3670, 0.4731, 0.3068, 0.3851, 0.2545, 0.3982, 0.3331, 0.3275, 0.2651,\n",
            "        0.2924, 0.2937, 0.3405, 0.1584, 0.4673, 0.3650, 0.3080, 0.3926, 0.3980,\n",
            "        0.4747, 0.2183, 0.3136, 0.3446, 0.3244, 0.3204, 0.2309, 0.2281, 0.2370,\n",
            "        0.4134, 0.3403, 0.2981, 0.3239, 0.3285, 0.2340, 0.2893, 0.2982, 0.3163,\n",
            "        0.2002, 0.1924, 0.4086, 0.3631, 0.2908, 0.3413, 0.4170, 0.2501, 0.3761,\n",
            "        0.2488, 0.4285, 0.2381, 0.3290, 0.3364, 0.4580, 0.2484, 0.4836, 0.3839,\n",
            "        0.2940, 0.3305, 0.2944, 0.2536, 0.2432, 0.3321, 0.3013, 0.2647, 0.2344,\n",
            "        0.2021, 0.2176, 0.2438, 0.2324, 0.2871, 0.3344, 0.3177, 0.3571, 0.2198,\n",
            "        0.3426, 0.3734, 0.3312, 0.3060, 0.2616, 0.4104, 0.3295, 0.2650, 0.2098,\n",
            "        0.3804, 0.2327, 0.2448, 0.2573, 0.2813, 0.4808, 0.2713, 0.3311, 0.2720,\n",
            "        0.1889, 0.2960, 0.2754, 0.2898, 0.3555, 0.5231, 0.2520, 0.4034, 0.3590,\n",
            "        0.4251, 0.2165, 0.4578, 0.3436, 0.3293, 0.4471, 0.2918, 0.3822, 0.3238,\n",
            "        0.1824, 0.2126, 0.3340, 0.3454, 0.2469, 0.1877, 0.2695, 0.2893, 0.3631,\n",
            "        0.2146, 0.3557, 0.2760, 0.2538, 0.3918, 0.2777, 0.2854, 0.3330, 0.1766,\n",
            "        0.9213, 0.2753, 0.2797, 0.3001, 0.2853, 0.2157, 0.2348, 0.2421, 0.3465,\n",
            "        0.2652, 0.3682, 0.2943, 0.5105, 0.1856, 0.3056, 0.2642, 0.3938, 0.3164,\n",
            "        0.4967, 0.3144, 0.4320, 0.3276, 0.2822, 0.2458, 0.2163, 0.2826, 0.3113,\n",
            "        0.4196, 0.2636, 0.4799, 0.3369, 0.3959, 0.4846, 0.1618, 0.3356, 0.5261,\n",
            "        0.2186, 0.3955, 0.2350, 0.2982, 0.4380, 0.1878, 0.2718, 0.1852, 0.3697,\n",
            "        0.3380, 0.3302, 0.3936, 0.3162, 0.2825, 0.4823, 0.3053, 0.2673, 0.3264,\n",
            "        0.5255, 0.4188, 0.3980, 0.2698, 0.4334, 0.2424, 0.2759, 0.3645, 0.2945,\n",
            "        0.4020, 0.2819, 0.3247, 0.3427, 0.2303, 0.2386, 0.3668, 0.2904, 0.3478,\n",
            "        0.2860, 0.2970, 0.2986, 0.2846, 0.4346, 0.2551, 0.2447, 0.2681, 0.3754,\n",
            "        0.2855, 0.4628, 0.3330, 0.2779, 0.4257, 0.2945, 0.2274, 0.4049, 0.4092,\n",
            "        0.2794, 0.3092, 0.3337, 0.4474, 0.3176, 0.3180, 0.2083, 0.2630, 0.3643,\n",
            "        0.1954, 0.2755, 0.3844, 0.3826, 0.1867, 0.2673, 0.3168, 0.3663, 0.3155,\n",
            "        0.3382, 0.3115, 0.3018, 0.2348, 0.2494, 0.2686, 0.2957, 0.2600, 0.3952,\n",
            "        0.3302, 0.4119, 0.1787, 0.2060, 0.3363, 0.3289, 0.2952, 0.2594, 0.2227,\n",
            "        0.2404, 0.1748, 0.2380, 0.3037, 0.1731, 0.3443, 0.2229, 0.3245, 0.3468,\n",
            "        0.3152, 0.4882, 0.2917, 0.2134, 0.4452, 0.3341, 0.3521, 0.2175, 0.3697,\n",
            "        0.3306, 0.4452, 0.4085, 0.4061, 0.2625, 0.2866, 0.3349, 0.1885, 0.2949,\n",
            "        0.3039, 0.2525, 0.4577, 0.3411, 0.2740, 0.3485, 0.2006, 0.1808, 0.3532,\n",
            "        0.2170, 0.3263, 0.3167, 0.3429, 0.2150, 0.3194, 0.2245, 0.2814, 0.2284,\n",
            "        0.2686, 0.3235, 0.3632, 0.3201, 0.1997, 0.3735, 0.2798, 0.2944, 0.2715,\n",
            "        0.2959, 0.2149, 0.3376, 0.2061, 0.3205, 0.2686, 0.3380, 0.4320, 0.2589,\n",
            "        0.2269, 0.2260, 0.6617, 0.3783, 0.3364, 0.3866, 0.2625, 0.1359, 0.4873,\n",
            "        0.4171, 0.2479, 0.1733, 0.4227, 0.5040, 0.2475, 0.3179, 0.3668, 0.2731,\n",
            "        0.2481, 0.3234, 0.4601, 0.4307, 0.2529, 0.3211, 0.3854, 0.2097, 0.1969,\n",
            "        0.3455, 0.2639, 0.3263, 0.5125, 0.4441, 0.2191, 0.2948, 0.2527, 0.2069,\n",
            "        0.3548, 0.2625, 0.3769, 0.3121, 0.2988, 0.2339, 0.2636, 0.2225],\n",
            "       device='cuda:0')\n",
            "rank [279 470   4  60 314 333 151 248 498 291 481 297 137 415 476 149 311 205\n",
            " 329 239 308 171  21 154  17 166  91  19 361 143 488  51 203 254 434 372\n",
            " 257   9 147 424 418 499  79 319 355  10 337 133  87 466 299  20  73 489\n",
            " 124 199  31 364 252 480 306 334 477 195  23 148 180 397 230 368 191 110\n",
            " 425  11 426 128 367 105 250  76 342  30 158 170 335 310 316 395 295 326\n",
            " 169  47 274 473 492 156 380 121  86 206 381 259 115 234 471 506 197 359\n",
            " 141 455 226  71   1 422 323  13 289  39 153 484 348  66 385 103 167 340\n",
            " 377 452 192 269  37 251  29 223  54 271 247  38 504 440 142 420  63  89\n",
            " 437  93 350 413 116 287 495 264 122 129 174 410 255 107  53 444 112 345\n",
            " 225  52  42  99 194 435 164 181   0 387 465 324 101 461 309  90 202 472\n",
            " 400  64 313  88 429 150 221   7 419 263 371 159 277 362 212 227 241 423\n",
            " 208 325 396 231 256 201 401  32 184 300 160  46 332 442 497 117 344  22\n",
            " 412 175 127 183  65 260 451 487 491 463 176 453 446  70  80 374 483 222\n",
            " 373 384 443 296 188 327 134 386 414 298 173 135 119 507 388 305 370  14\n",
            " 168 155 228 293 330  55 432 408 389 213  67 282 508  78 353 318 187 182\n",
            " 132  77 352  58  41 244 459 393 402 431  27 501 365 341 457 209 290  81\n",
            " 207 163 114  25 162  83  95 258 416 131 108 193   3 349 246 186 268 104\n",
            " 113 220 428 351 360 276 283 354 304 328   8 111 301 343 448 238 456 281\n",
            " 369  62  84 363 275 106 102 272 125 339 379 245 280  82 436 485 242 321\n",
            " 458 240 336 267 140  36  49 450 464 392 358 383 331 138 123  72 118 288\n",
            " 161 232 214 294 496 307 510 376 474 505 427  45  50 229  18  94 394  98\n",
            " 403 467 237 145  92 356  16 157 273 210 490 502 433 249  44 196  33 391\n",
            "  57 198 204 486  69 478  35 482 265 146   2 302  40 130 236 357 218 211\n",
            " 338 286  24   6 405  68 347 200 407 179  34  85 317 390 285 215 185 509\n",
            "  48 235 219 177 346  56 449 178 366 468  59 469  28 447  43 411 404 511\n",
            " 100 224 500 315 172 217 421 441 253 303 284 445 460 270  74 417 262  75\n",
            " 233  12 493  96 375  97 503 462 399 216 144 438 189 454 494  61 378 190\n",
            " 243 430 320 266 382 139 136 292 322  26 261 439 398 109 278 152 406 479\n",
            " 409 126   5 312 120 165  15 475]\n",
            "Processing layer 10, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.2165, 0.1655, 0.1092, 0.1118, 0.2380, 0.2389, 0.1333, 0.0821, 0.2683,\n",
            "        0.2058, 0.2377, 0.2133, 0.1248, 0.2224, 0.1343, 0.1005, 0.2657, 0.4099,\n",
            "        0.2388, 0.1221, 0.3037, 0.1611, 0.0904, 0.2301, 0.3601, 0.1127, 0.1877,\n",
            "        0.1501, 0.2677, 0.1764, 0.3266, 0.2333, 0.1820, 0.2803, 0.1558, 0.3600,\n",
            "        0.2290, 0.1672, 0.1394, 0.2763, 0.2510, 0.1701, 0.2677, 0.0868, 0.2121,\n",
            "        0.2786, 0.1288, 0.1214, 0.1610, 0.1002, 0.1176, 0.2254, 0.1905, 0.1733,\n",
            "        0.1800, 0.1010, 0.2135, 0.0969, 0.3816, 0.2039, 0.3446, 0.1760, 0.1438,\n",
            "        0.2137, 0.2418, 0.2570, 0.2070, 0.2946, 0.1878, 0.1347, 0.1832, 0.3564,\n",
            "        0.3204, 0.1742, 0.1397, 0.0489, 0.2968, 0.1125, 0.2252, 0.2224, 0.1814,\n",
            "        0.2664, 0.2635, 0.1027, 0.1005, 0.1399, 0.1923, 0.2079, 0.3338, 0.2508,\n",
            "        0.2665, 0.3792, 0.1791, 0.1538, 0.3715, 0.1575, 0.2119, 0.2816, 0.3391,\n",
            "        0.2550, 0.2372, 0.1549, 0.1122, 0.1947, 0.1386, 0.0769, 0.1901, 0.2958,\n",
            "        0.1508, 0.2337, 0.1588, 0.4090, 0.3452, 0.1623, 0.4358, 0.1049, 0.3342,\n",
            "        0.2395, 0.2031, 0.2206, 0.2802, 0.1562, 0.1425, 0.1442, 0.2523, 0.2203,\n",
            "        0.1787, 0.4532, 0.0960, 0.2759, 0.1547, 0.2078, 0.0993, 0.2362, 0.1569,\n",
            "        0.1109, 0.1845, 0.2012, 0.2896, 0.2719, 0.2123, 0.2282, 0.3685, 0.1502,\n",
            "        0.0659, 0.2880, 0.2420, 0.2299, 0.1102, 0.2631, 0.1682, 0.1409, 0.1908,\n",
            "        0.2876, 0.1063, 0.2210, 0.1816, 0.2166, 0.0962, 0.0794, 0.2899, 0.2388,\n",
            "        0.0555, 0.1606, 0.1050, 0.3117, 0.1453, 0.1414, 0.3491, 0.3401, 0.1995,\n",
            "        0.1895, 0.3286, 0.1538, 0.1954, 0.4089, 0.1919, 0.1675, 0.2139, 0.2499,\n",
            "        0.2051, 0.3919, 0.1265, 0.2362, 0.1870, 0.2205, 0.2274, 0.2172, 0.1140,\n",
            "        0.0944, 0.1862, 0.2680, 0.2937, 0.2302, 0.1482, 0.1862, 0.0976, 0.1570,\n",
            "        0.1990, 0.3405, 0.0883, 0.2558, 0.1107, 0.1063, 0.1117, 0.0954, 0.1026,\n",
            "        0.1782, 0.0648, 0.2746, 0.4012, 0.0670, 0.2031, 0.1756, 0.2209, 0.1273,\n",
            "        0.1179, 0.3517, 0.1222, 0.0823, 0.2724, 0.2197, 0.2910, 0.0852, 0.1573,\n",
            "        0.1489, 0.0824, 0.1919, 0.1820, 0.2420, 0.1912, 0.2863, 0.2785, 0.1404,\n",
            "        0.2381, 0.2252, 0.2125, 0.2367, 0.2020, 0.2916, 0.1895, 0.1673, 0.3414,\n",
            "        0.1924, 0.1661, 0.2618, 0.1180, 0.0995, 0.1404, 0.1727, 0.1393, 0.1220,\n",
            "        0.2324, 0.1235, 0.1920, 0.2933, 0.1586, 0.2963, 0.2203, 0.1226, 0.1027,\n",
            "        0.2459, 0.1462, 0.2878, 0.2133, 0.1224, 0.1854, 0.3591, 0.1633, 0.1967,\n",
            "        0.2107, 0.1459, 0.4421, 0.3009, 0.2093, 0.1812, 0.2000, 0.0985, 0.2205,\n",
            "        0.1621, 0.0799, 0.2245, 0.1512, 0.2640, 0.2263, 0.1944, 0.2204, 0.1178,\n",
            "        0.2142, 0.2485, 0.0793, 0.3686, 0.1405, 0.2744, 0.1416, 0.2696, 0.1226,\n",
            "        0.3804, 0.0989, 0.1955, 0.1083, 0.3375, 0.2920, 0.1262, 0.2096, 0.2392,\n",
            "        0.2126, 0.1531, 0.1636, 0.2122, 0.0848, 0.2236, 0.1530, 0.1830, 0.2063,\n",
            "        0.2679, 0.1280, 0.2311, 0.1262, 0.2328, 0.2119, 0.1670, 0.2110, 0.1326,\n",
            "        0.1953, 0.1207, 0.0880, 0.2335, 0.3178, 0.2927, 0.1611, 0.1807, 0.1313,\n",
            "        0.4765, 0.1823, 0.2347, 0.2227, 0.1427, 0.2736, 0.1776, 0.1888, 0.4214,\n",
            "        0.1304, 0.2957, 0.1859, 0.1460, 0.1383, 0.3673, 0.1429, 0.1578, 0.1996,\n",
            "        0.1358, 0.1826, 0.1484, 0.2606, 0.1510, 0.1857, 0.2155, 0.2175, 0.2214,\n",
            "        0.1008, 0.1107, 0.0933, 0.3204, 0.1807, 0.2247, 0.2056, 0.1611, 0.1865,\n",
            "        0.1965, 0.1686, 0.2092, 0.0789, 0.2313, 0.1867, 0.1220, 0.2238, 0.1288,\n",
            "        0.1928, 0.2094, 0.2532, 0.2768, 0.1445, 0.2680, 0.2642, 0.2168, 0.3820,\n",
            "        0.0649, 0.3526, 0.1948, 0.1799, 0.2422, 0.2405, 0.1740, 0.1014, 0.1286,\n",
            "        0.1768, 0.2527, 0.2948, 0.3333, 0.3238, 0.1592, 0.1432, 0.2678, 0.1178,\n",
            "        0.2457, 0.3535, 0.0707, 0.2074, 0.2488, 0.2214, 0.2608, 0.1548, 0.1666,\n",
            "        0.1045, 0.3114, 0.2381, 0.3038, 0.1556, 0.1822, 0.1309, 0.4459, 0.2837,\n",
            "        0.1509, 0.1105, 0.0923, 0.2723, 0.1655, 0.1620, 0.2042, 0.1761, 0.1369,\n",
            "        0.2603, 0.1753, 0.1184, 0.2217, 0.1888, 0.1617, 0.2324, 0.2689, 0.2479,\n",
            "        0.2018, 0.0935, 0.2197, 0.1987, 0.1445, 0.1263, 0.2339, 0.1896, 0.1784,\n",
            "        0.2743, 0.2026, 0.2446, 0.1539, 0.1043, 0.2664, 0.0895, 0.2743, 0.2060,\n",
            "        0.1165, 0.1717, 0.3347, 0.2431, 0.2176, 0.1585, 0.1810, 0.2375, 0.1424,\n",
            "        0.2585, 0.3107, 0.2798, 0.2463, 0.1716, 0.1164, 0.1523, 0.1790, 0.1492,\n",
            "        0.2195, 0.0838, 0.1493, 0.0556, 0.1804, 0.1351, 0.3165, 0.0860, 0.3494,\n",
            "        0.1016, 0.0661, 0.2375, 0.2051, 0.2948, 0.2549, 0.1356, 0.0216, 0.1541,\n",
            "        0.0759, 0.2579, 0.1702, 0.2143, 0.2738, 0.2273, 0.2417, 0.1046, 0.2285,\n",
            "        0.1642, 0.2096, 0.1603, 0.2183, 0.1828, 0.1878, 0.2456, 0.2340],\n",
            "       device='cuda:0')\n",
            "rank [333 127 421 272 114 341  17 111 175 210 181 386  58 297  91  94 291 142\n",
            " 347  24  35 267  71 406 388 217 485 168 112  60 242 199 169  98 301 461\n",
            " 116  88 399 172  30 400  72 363 328 483 165 415 469 417  20 273  76 257\n",
            " 107 343 398 490  67 192 255 329 302 239 222 160 138 145 263 153 231 422\n",
            "  97  33 120 470  45 232 381  39 129 209 293 450 457 499 338 220 426 139\n",
            " 295 439   8 383 191 315 403  42  28  90 455  81  16 384 283  82 149 245\n",
            " 411 354 432 468 496  65 201  99 491 380 397 124  40  89 179 409 289 440\n",
            " 471 261 405 510 452 462 391 229 146  64 501 392 117 305   5 161  18 234\n",
            " 416   4  10 466 488 100 237 183 133 335 511 447 109 327  31 319 438 252\n",
            " 373 317 193  23 147  36 503 141 186 500 284  51 235  78 365 281 376 311\n",
            " 336  13  79 435 410 359 155 214 119 185 278 286 258 125 443 221 477 507\n",
            " 463 358 187 385 157   0 357 498 288 178  63  56 264  11 306 236 140 309\n",
            "  44 320  96 322 270 304 505 379 274 371  87 131 408  66 314 458   9 366\n",
            " 180 489 429  59 212 118 451 238 441 137 276 350 170 198 444 269 369 299\n",
            " 174 324 389 103 285 378 243  86 254 227 176 230 152  52 106 448 240 171\n",
            " 340 436 509  68  26 184 374 368 190 195 344 356 266 136  70 313 508 352\n",
            " 334 419  32 228 156  80 275 465 364 331 481  54 390  92 475 126 449 207\n",
            " 339 396  29 430  61 213 433  73 393  53 249 460 472 497  41 370 150 177\n",
            " 241  37 321 413 244 427   1 504 308 268 113 279 428 437  21 367 330  48\n",
            " 163 506 401 110 256 464 349  95 224 197 134 121  34 418 101 412 130 494\n",
            " 453  93 173 307 312 474 282 355 423 108 143  27 479 476 225 353 194 262\n",
            " 345 271 166 445 382 123  62 402 348 337 122 467 294 167 151 292 233 248\n",
            "  85  74  38 250 104 346 431 351 492 482  69  14   6 323 332 420 342 377\n",
            "  46 395 316 215 182 446 318 303  12 253 259 296 265 218  19 375 251  47\n",
            " 325 434 246 216 287 404  50 459 473 188  25  77 102   3 204 135 361 202\n",
            " 424 148   2 300 154 203 164 115 502 414 454  83 260 206 486 394  55 360\n",
            "  15  84  49 247 132 298 277 196  57 158 128 205 189 442 362 425  22 456\n",
            " 200 326  43 484 223 310 478 226 219   7 280 159 290 372 105 495 407 211\n",
            " 487 144 387 208 480 162  75 493]\n",
            "Processing layer 11, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.0951, 0.2325, 0.0961, 0.0995, 0.1230, 0.1520, 0.1451, 0.1648, 0.1684,\n",
            "        0.1152, 0.1516, 0.1674, 0.1253, 0.0473, 0.1736, 0.1024, 0.1095, 0.1521,\n",
            "        0.1869, 0.0708, 0.1774, 0.1469, 0.1574, 0.1329, 0.1172, 0.1392, 0.1310,\n",
            "        0.1913, 0.1274, 0.1891, 0.1134, 0.1324, 0.1127, 0.1602, 0.1473, 0.1023,\n",
            "        0.1027, 0.1051, 0.1289, 0.1193, 0.1575, 0.1064, 0.1220, 0.1498, 0.1549,\n",
            "        0.1070, 0.2074, 0.1408, 0.1239, 0.0965, 0.1297, 0.1076, 0.1724, 0.1028,\n",
            "        0.1771, 0.2038, 0.1374, 0.0953, 0.1312, 0.0592, 0.1064, 0.1122, 0.1218,\n",
            "        0.1319, 0.1561, 0.1076, 0.0869, 0.1221, 0.0705, 0.0787, 0.1687, 0.1294,\n",
            "        0.0714, 0.0919, 0.1055, 0.1457, 0.1504, 0.1462, 0.1801, 0.1244, 0.1582,\n",
            "        0.0863, 0.1014, 0.1823, 0.1173, 0.2035, 0.0918, 0.1653, 0.1128, 0.1883,\n",
            "        0.1847, 0.2312, 0.1252, 0.1437, 0.0830, 0.1233, 0.0948, 0.0975, 0.1411,\n",
            "        0.0840, 0.0993, 0.1100, 0.0968, 0.2261, 0.1211, 0.1070, 0.1354, 0.0724,\n",
            "        0.1170, 0.0879, 0.0853, 0.0889, 0.2028, 0.1429, 0.1201, 0.1642, 0.1424,\n",
            "        0.1139, 0.1780, 0.0920, 0.1984, 0.1178, 0.1439, 0.1523, 0.0926, 0.1143,\n",
            "        0.1415, 0.1405, 0.1045, 0.0859, 0.1024, 0.2357, 0.1473, 0.1557, 0.1196,\n",
            "        0.0773, 0.1121, 0.1401, 0.1543, 0.1029, 0.1961, 0.0870, 0.1515, 0.1034,\n",
            "        0.0831, 0.1188, 0.1290, 0.1538, 0.0918, 0.1555, 0.1137, 0.0983, 0.0893,\n",
            "        0.1163, 0.1823, 0.1173, 0.1273, 0.1027, 0.1022, 0.2124, 0.0913, 0.0674,\n",
            "        0.1704, 0.1065, 0.1611, 0.0807, 0.1499, 0.1146, 0.1474, 0.1202, 0.1571,\n",
            "        0.2203, 0.1003, 0.1300, 0.1562, 0.1840, 0.0865, 0.1317, 0.1776, 0.0883,\n",
            "        0.1658, 0.1164, 0.1709, 0.1272, 0.0661, 0.0961, 0.0820, 0.2089, 0.1306,\n",
            "        0.1703, 0.1461, 0.1061, 0.1674, 0.1289, 0.1589, 0.1321, 0.1213, 0.1441,\n",
            "        0.1141, 0.1978, 0.2767, 0.0922, 0.1302, 0.1029, 0.1886, 0.2269, 0.1707,\n",
            "        0.1339, 0.0668, 0.1365, 0.2038, 0.1161, 0.1300, 0.1238, 0.1757, 0.1209,\n",
            "        0.0909, 0.0941, 0.1636, 0.1378, 0.0975, 0.0845, 0.1810, 0.0828, 0.1290,\n",
            "        0.2150, 0.1252, 0.0923, 0.0951, 0.1488, 0.1001, 0.1052, 0.1324, 0.1633,\n",
            "        0.1545, 0.0840, 0.1121, 0.0992, 0.2234, 0.0870, 0.0934, 0.0955, 0.1533,\n",
            "        0.1644, 0.1093, 0.1007, 0.1274, 0.1250, 0.1167, 0.1116, 0.1889, 0.1886,\n",
            "        0.1348, 0.1723, 0.0900, 0.1508, 0.1188, 0.2085, 0.1025, 0.1903, 0.1646,\n",
            "        0.1177, 0.1737, 0.1821, 0.1429, 0.1001, 0.0903, 0.1458, 0.0873, 0.0885,\n",
            "        0.1017, 0.1336, 0.0839, 0.1479, 0.1132, 0.1913, 0.1258, 0.1658, 0.1130,\n",
            "        0.0945, 0.0851, 0.1787, 0.1034, 0.1393, 0.1397, 0.0933, 0.1132, 0.0985,\n",
            "        0.0881, 0.1495, 0.1078, 0.1290, 0.1664, 0.1324, 0.1427, 0.1028, 0.2257,\n",
            "        0.1353, 0.0974, 0.1210, 0.0804, 0.1573, 0.1608, 0.1272, 0.1371, 0.1013,\n",
            "        0.1377, 0.2369, 0.1529, 0.1491, 0.1541, 0.1452, 0.1230, 0.1593, 0.1128,\n",
            "        0.1778, 0.1248, 0.1413, 0.1480, 0.1546, 0.1860, 0.1067, 0.1049, 0.1042,\n",
            "        0.1463, 0.1103, 0.1185, 0.1113, 0.1472, 0.1428, 0.1108, 0.1132, 0.1047,\n",
            "        0.1559, 0.0958, 0.1261, 0.1310, 0.1553, 0.1204, 0.1082, 0.1669, 0.1710,\n",
            "        0.1428, 0.1805, 0.2099, 0.1159, 0.1199, 0.1357, 0.1058, 0.1375, 0.1564,\n",
            "        0.1199, 0.1571, 0.2017, 0.0725, 0.0884, 0.1029, 0.1267, 0.2013, 0.1101,\n",
            "        0.2440, 0.1710, 0.1473, 0.1259, 0.2102, 0.1374, 0.1649, 0.0964, 0.1164,\n",
            "        0.1128, 0.0949, 0.1347, 0.0822, 0.0993, 0.1561, 0.1206, 0.2059, 0.1445,\n",
            "        0.1349, 0.1216, 0.1465, 0.1107, 0.1417, 0.2153, 0.1484, 0.1320, 0.1708,\n",
            "        0.1127, 0.1149, 0.0832, 0.1752, 0.1099, 0.1008, 0.1294, 0.1111, 0.1578,\n",
            "        0.1050, 0.2039, 0.1187, 0.1115, 0.1476, 0.1696, 0.1331, 0.1138, 0.0765,\n",
            "        0.2005, 0.1222, 0.1696, 0.1177, 0.1003, 0.1690, 0.0845, 0.1558, 0.1258,\n",
            "        0.1260, 0.0865, 0.1162, 0.1532, 0.1212, 0.1723, 0.1053, 0.0916, 0.2380,\n",
            "        0.1319, 0.1416, 0.1302, 0.1313, 0.1740, 0.1196, 0.0925, 0.1695, 0.1427,\n",
            "        0.1307, 0.1386, 0.0901, 0.0547, 0.1298, 0.1765, 0.0760, 0.1272, 0.1624,\n",
            "        0.0739, 0.1076, 0.0994, 0.0988, 0.1140, 0.0970, 0.0981, 0.1077, 0.1260,\n",
            "        0.1242, 0.0926, 0.1125, 0.1506, 0.1453, 0.1213, 0.1058, 0.1819, 0.0737,\n",
            "        0.1075, 0.1940, 0.0761, 0.0874, 0.1616, 0.1002, 0.0943, 0.1220, 0.1384,\n",
            "        0.1318, 0.1199, 0.1748, 0.1014, 0.0921, 0.2048, 0.1606, 0.1907, 0.1241,\n",
            "        0.1434, 0.1355, 0.1089, 0.1070, 0.1469, 0.1107, 0.1223, 0.0846, 0.1301,\n",
            "        0.1023, 0.1084, 0.1902, 0.1590, 0.1513, 0.1813, 0.0863, 0.1296, 0.1501,\n",
            "        0.1167, 0.1120, 0.1510, 0.1531, 0.0760, 0.1323, 0.1284, 0.0942, 0.2079,\n",
            "        0.1167, 0.1280, 0.0942, 0.1309, 0.1370, 0.1489, 0.1143, 0.0497],\n",
            "       device='cuda:0')\n",
            "rank [200 360 422 307 131   1  91 205 103 296 238 171 383 225 159 364 344 187\n",
            " 257 503  46 376 473 397 210  55  85 112 353 358 405 120 199 140 460  27\n",
            " 275 475 259 488  29 250 251 204  89  18 320  90 175  83 154 263 457 491\n",
            " 222 343  78 281 118 315 178  20  54 437 214 390 470 427 262  14  52 253\n",
            " 419 361 341 182 386 206 162 189 407 401 430 410  70   8  11 192 340 292\n",
            " 180 277  87 366   7 260 243 115 218 233 440 463 164 302 474  33 313 489\n",
            " 194  80 395  40  22 301 352 170 350 174 374  64 333 412 133 149 337  44\n",
            " 319 234 138 310 147 242 417 498 308 123  17   5  10 142 490 497 255 453\n",
            "  76 494 166  43 289 309 509 229 384 318 273 400 168  34 132 362 328  21\n",
            " 481 380 324  77 190 267  75 454 311   6 377 197 122  93 477 113 264 329\n",
            " 342 294 431 116 382 424 126 317  98  47 127 137 284 283  25 433 467 219\n",
            " 306 349  56 365 304 508 209 347 478 106 297 378 252 371 207 271 402  23\n",
            "  31 232 293 500 195 385  63 423 468 177 426  58 336  26 507 432 188 425\n",
            " 202 485 173 212 436  50 493 393  71 146 291 224 193  38 501 505  28 246\n",
            " 156 183 439 303 357 335 449 414 363 276 413  12  92 226 247 316  79 450\n",
            " 476  48 213  95 312   4 483 406  67  42 466  62 379 455 196 418 104 299\n",
            " 215 375 338 169 114 346 469 351 428 134  39 145 256 398 326 121 261 408\n",
            " 155  84  24 108 248 495 504 368 181 153 416 211 345   9 388 167 510 125\n",
            " 198 445 117 403 150  30 274 331 286 278 314 369  88  32 387 452  61 236\n",
            " 136 496 249 399 327 394 330 482 381 325 359 101 391  16 244 479 487 339\n",
            " 290 448 442  65  51 459  45 105 480 321 163  60  41 191 348 456  74 420\n",
            " 231  37 396 322 332 128 323 143 282 356 203 139  53 295 157  36 258  15\n",
            " 130 486  35 158 270 471  82 305 392 245 409 172 464 265 230   3 443 373\n",
            " 100 237 444 287 151 447  97 220 298 446 102  49 367 185   2 334 241  57\n",
            "   0 228 370  96 279 465 502 506 217 240 285 451 124 429 227 201 472 119\n",
            "  73  86 148 421 160 216 266 434 254 152 111 269 355 179 288 109 462 268\n",
            " 141 239  66 415 176 492  81 129 110 280 484 221 411  99 235 272 389 144\n",
            "  94 223 372 186 165 300  69 135 404 461 438 499 441 458 354 107  72  19\n",
            "  68 161 208 184  59 435 511  13]\n",
            "Processing layer 12, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.1113, 0.1202, 0.0747, 0.1639, 0.1122, 0.0804, 0.1237, 0.1653, 0.0645,\n",
            "        0.0994, 0.1004, 0.0996, 0.1222, 0.1242, 0.0977, 0.0709, 0.0809, 0.1125,\n",
            "        0.0975, 0.0950, 0.1096, 0.1232, 0.1498, 0.0997, 0.1006, 0.0687, 0.1178,\n",
            "        0.0629, 0.0908, 0.0908, 0.1247, 0.1389, 0.0761, 0.0641, 0.0851, 0.0852,\n",
            "        0.1118, 0.1336, 0.0966, 0.1309, 0.0827, 0.1320, 0.0851, 0.1443, 0.1230,\n",
            "        0.0530, 0.1121, 0.1084, 0.0790, 0.1525, 0.1056, 0.0967, 0.1534, 0.1196,\n",
            "        0.0970, 0.0776, 0.1231, 0.1229, 0.1087, 0.1039, 0.0863, 0.0591, 0.1065,\n",
            "        0.0736, 0.0827, 0.0755, 0.0916, 0.1177, 0.0696, 0.1330, 0.1254, 0.0719,\n",
            "        0.1054, 0.1565, 0.1438, 0.0975, 0.1313, 0.1029, 0.0896, 0.0584, 0.0903,\n",
            "        0.1058, 0.1726, 0.1201, 0.0839, 0.0811, 0.1441, 0.0796, 0.1125, 0.0874,\n",
            "        0.1233, 0.1144, 0.0795, 0.1156, 0.1030, 0.1447, 0.1427, 0.0950, 0.1183,\n",
            "        0.0937, 0.1397, 0.1065, 0.1333, 0.1178, 0.0701, 0.0625, 0.1104, 0.1737,\n",
            "        0.1006, 0.0792, 0.1265, 0.1247, 0.1295, 0.1058, 0.2030, 0.0922, 0.0934,\n",
            "        0.0787, 0.0769, 0.0843, 0.1072, 0.0843, 0.1105, 0.1260, 0.1146, 0.1268,\n",
            "        0.1261, 0.1775, 0.1683, 0.1141, 0.1060, 0.1557, 0.0805, 0.1071, 0.0859,\n",
            "        0.1477, 0.1625, 0.1394, 0.0741, 0.1894, 0.0861, 0.1278, 0.1618, 0.0992,\n",
            "        0.0935, 0.0868, 0.1311, 0.1284, 0.1409, 0.1585, 0.1076, 0.1794, 0.0976,\n",
            "        0.0907, 0.0923, 0.1067, 0.0951, 0.1463, 0.1088, 0.1338, 0.1759, 0.1438,\n",
            "        0.1155, 0.0952, 0.0801, 0.1271, 0.1626, 0.1369, 0.1206, 0.1174, 0.1068,\n",
            "        0.0854, 0.1196, 0.0851, 0.1091, 0.1645, 0.1382, 0.1301, 0.0834, 0.1086,\n",
            "        0.1675, 0.1267, 0.1304, 0.0577, 0.1243, 0.0594, 0.1063, 0.1424, 0.1160,\n",
            "        0.0538, 0.2079, 0.1078, 0.0653, 0.1028, 0.0827, 0.1412, 0.0832, 0.1228,\n",
            "        0.1193, 0.1449, 0.0995, 0.0702, 0.1388, 0.1183, 0.1035, 0.1296, 0.1352,\n",
            "        0.1050, 0.0813, 0.0534, 0.1231, 0.0971, 0.1140, 0.1318, 0.1307, 0.1121,\n",
            "        0.0872, 0.0682, 0.1380, 0.1064, 0.0891, 0.0696, 0.1233, 0.0978, 0.1055,\n",
            "        0.1234, 0.0726, 0.0841, 0.0736, 0.0971, 0.1254, 0.0887, 0.1104, 0.0921,\n",
            "        0.0906, 0.1082, 0.1414, 0.0937, 0.0963, 0.1140, 0.1155, 0.1298, 0.1196,\n",
            "        0.0814, 0.0901, 0.1201, 0.1102, 0.0719, 0.1495, 0.1009, 0.0957, 0.1127,\n",
            "        0.1735, 0.1273, 0.1115, 0.1055, 0.0877, 0.0935, 0.1209, 0.1350, 0.1460,\n",
            "        0.1248, 0.1100, 0.1051, 0.1167, 0.0986, 0.1201, 0.0778, 0.1435, 0.1282,\n",
            "        0.0869, 0.1124, 0.1121, 0.0968, 0.1323, 0.1137, 0.0827, 0.0722, 0.0775,\n",
            "        0.1401, 0.1146, 0.0983, 0.1221, 0.0791, 0.0937, 0.1874, 0.0890, 0.0715,\n",
            "        0.1156, 0.1351, 0.1535, 0.0841, 0.0961, 0.0816, 0.1037, 0.0725, 0.1775,\n",
            "        0.1361, 0.1041, 0.1047, 0.0868, 0.0873, 0.0912, 0.1235, 0.1087, 0.0911,\n",
            "        0.0983, 0.1303, 0.1042, 0.0373, 0.0675, 0.1008, 0.1094, 0.0966, 0.1410,\n",
            "        0.1079, 0.0989, 0.0671, 0.1331, 0.1900, 0.0946, 0.0813, 0.1000, 0.1249,\n",
            "        0.1704, 0.1094, 0.0974, 0.1047, 0.1215, 0.0881, 0.0938, 0.0739, 0.0676,\n",
            "        0.1411, 0.1260, 0.1788, 0.1111, 0.1266, 0.1446, 0.0971, 0.1037, 0.0963,\n",
            "        0.1171, 0.1665, 0.1079, 0.1519, 0.0980, 0.1330, 0.1080, 0.1282, 0.1657,\n",
            "        0.0865, 0.1163, 0.1316, 0.1523, 0.1479, 0.1315, 0.1460, 0.0905, 0.1078,\n",
            "        0.1181, 0.1307, 0.0931, 0.0996, 0.1487, 0.0904, 0.0917, 0.0833, 0.1052,\n",
            "        0.0995, 0.1328, 0.0957, 0.0833, 0.0917, 0.1205, 0.1010, 0.1224, 0.1235,\n",
            "        0.1472, 0.0817, 0.1219, 0.0593, 0.1196, 0.1691, 0.0889, 0.1227, 0.1086,\n",
            "        0.1498, 0.0582, 0.1393, 0.1246, 0.0929, 0.1445, 0.1250, 0.0731, 0.1183,\n",
            "        0.0967, 0.0747, 0.1193, 0.0910, 0.1159, 0.1609, 0.1557, 0.0954, 0.1023,\n",
            "        0.1349, 0.0953, 0.1884, 0.0933, 0.1130, 0.1260, 0.0892, 0.1173, 0.1090,\n",
            "        0.1276, 0.1316, 0.1359, 0.1503, 0.1553, 0.0754, 0.1044, 0.0932, 0.1369,\n",
            "        0.1344, 0.1047, 0.1061, 0.1004, 0.0634, 0.1046, 0.0998, 0.1031, 0.1262,\n",
            "        0.1379, 0.0671, 0.0938, 0.2008, 0.1051, 0.1563, 0.1206, 0.0718, 0.1641,\n",
            "        0.0635, 0.1304, 0.1050, 0.1332, 0.1381, 0.0842, 0.0732, 0.1201, 0.0705,\n",
            "        0.0878, 0.1108, 0.1080, 0.0711, 0.1449, 0.1257, 0.0879, 0.2023, 0.1259,\n",
            "        0.1024, 0.1105, 0.0931, 0.0971, 0.1464, 0.1202, 0.0783, 0.0808, 0.1207,\n",
            "        0.1598, 0.0817, 0.0914, 0.0976, 0.0976, 0.0994, 0.0880, 0.0799, 0.1082,\n",
            "        0.1244, 0.1220, 0.1550, 0.1809, 0.1228, 0.1703, 0.0894, 0.1076, 0.1176,\n",
            "        0.1092, 0.0986, 0.1068, 0.1297, 0.0810, 0.0815, 0.1048, 0.1038, 0.1772,\n",
            "        0.1913, 0.0857, 0.1128, 0.1373, 0.1145, 0.1516, 0.1179, 0.0616, 0.0743,\n",
            "        0.0959, 0.1179, 0.1183, 0.0974, 0.1131, 0.1300, 0.1442, 0.0901],\n",
            "       device='cuda:0')\n",
            "rank [190 114 457 435 495 319 139 407 285 480 151 335 127 296 494 160 107 252\n",
            "  82 324 482 383 128 180 343 350   7 175 440   3 166 136 142 401 468 149\n",
            "  73 437 131 402 418 479 290  52  49 354 345 500 417 387  22 248 364 355\n",
            " 135 378 463 157 260 357 454 199  95 338 392  43 510  86 161  74 268  96\n",
            " 187 236 195 333 314 148 279 100 137 389  31 202 176 445 218 432 498 167\n",
            " 422 297 416 206 289 259 405 423 159  37 102 444 318 347  69 370 274  41\n",
            " 213 415 353 356  76 146  39 214 361 442 182 307 177 509 241 489 205 112\n",
            " 147 349 269 141 414 253 165 125 181 337 110 431 126 123 410 334 458 455\n",
            " 230  70 393 323 261  30 111 390 477 184  13   6 303 377 225 222  90  21\n",
            "  56 210  44  57 197 481 385 376  12 282 478 380 328 258 467 168 438 374\n",
            " 464   1 266 448 245  83 382 242 172  53 398 198 203 395 506  98 360 505\n",
            " 501 103  26  67 485 169 412 342 264 352 188 400  93 288 240 162 280 124\n",
            " 499  91 129 239 212 275 508 409 497 251  88  17 271   4 272 215  46  36\n",
            " 254   0 336 451 460 122 232 106 246 262  20 312 325 486 174 413 158 304\n",
            "  58 386 179  47 235 476 452 348 315 344 191 359 150 484 120 133 488 170\n",
            " 155  62 101 219 186 425 130 113  81  50 224 255  72 368 436 263 207 443\n",
            " 492 424 327 299 428 420 308 298  59 493 340 294 204 430  94  77 193 459\n",
            " 404 375 249 311 108  24  10 426 322 429  23 363  11 200 369 473   9 143\n",
            " 316 265 487 306 281 346 223  14 472 471 152  18  75 507 326 211 339 462\n",
            " 229  54 273  51 396  38 313 341 238 292 504 250 371 403 406 163 156  19\n",
            "  97 320 434 330 237 284  99 144 257 116 408 421 362 461 391 154 115 233\n",
            " 366 373  66 470 302 305 399  29  28 153 234 358 365  80 244 511  78 483\n",
            " 411 220 286 384 231 329 474 456 450 256  89 301 216 270 300 145 351  60\n",
            " 140 134 496 171  35 173  42  34 121 119 446 227 291  84 178 367 372 196\n",
            " 194  64  40 276 469 379 293 491 243 321 208  85 490  16 466 132   5 164\n",
            " 475  87  92 109 283  48 117 465 267  55 278 118  32  65 419   2 397 503\n",
            " 138 331  63 228 447 394 226 295 277 247  71 439 287 453  15 449 201 104\n",
            "  68 221  25 217 332 310 317 433 192   8  33 441 427  27 105 502 185 381\n",
            "  61  79 388 183 189 209  45 309]\n",
            "Processing layer 13, original layer has 512 filters, pruning model has 384 filters\n",
            "torch.Size([512, 512, 3, 3])\n",
            "tensor([0.0939, 0.1464, 0.1735, 0.1191, 0.0876, 0.0956, 0.0803, 0.1136, 0.1021,\n",
            "        0.0886, 0.0798, 0.0565, 0.0771, 0.1121, 0.0886, 0.1321, 0.0897, 0.0893,\n",
            "        0.1254, 0.1145, 0.0757, 0.0773, 0.1128, 0.0897, 0.0765, 0.0771, 0.0982,\n",
            "        0.1377, 0.1079, 0.1418, 0.1043, 0.1378, 0.1426, 0.0854, 0.1273, 0.0983,\n",
            "        0.1007, 0.1274, 0.1020, 0.0985, 0.1326, 0.1535, 0.0945, 0.1628, 0.0721,\n",
            "        0.0786, 0.1340, 0.1156, 0.1076, 0.0924, 0.1394, 0.0818, 0.0917, 0.0913,\n",
            "        0.1089, 0.1080, 0.1689, 0.1264, 0.0490, 0.0978, 0.1032, 0.1009, 0.1025,\n",
            "        0.1051, 0.1256, 0.1243, 0.1090, 0.0894, 0.0749, 0.1069, 0.1435, 0.1930,\n",
            "        0.0932, 0.2054, 0.1406, 0.1741, 0.1179, 0.0871, 0.0832, 0.0928, 0.0782,\n",
            "        0.1002, 0.0894, 0.1294, 0.0883, 0.1098, 0.0768, 0.0929, 0.0805, 0.1208,\n",
            "        0.1323, 0.1488, 0.1189, 0.1130, 0.1290, 0.1947, 0.0914, 0.1716, 0.2356,\n",
            "        0.1498, 0.1058, 0.1045, 0.1048, 0.1676, 0.1368, 0.1543, 0.0663, 0.0915,\n",
            "        0.0958, 0.0793, 0.0905, 0.0785, 0.0845, 0.1229, 0.1458, 0.0532, 0.1230,\n",
            "        0.0701, 0.1953, 0.0591, 0.1041, 0.1104, 0.0981, 0.1609, 0.1330, 0.1410,\n",
            "        0.1051, 0.1666, 0.0934, 0.1153, 0.2215, 0.0878, 0.1382, 0.0952, 0.1197,\n",
            "        0.0777, 0.1122, 0.2139, 0.1682, 0.0695, 0.0963, 0.0913, 0.0881, 0.0651,\n",
            "        0.0647, 0.0697, 0.1107, 0.0791, 0.0963, 0.1047, 0.0826, 0.1135, 0.0684,\n",
            "        0.2029, 0.1395, 0.1167, 0.1447, 0.1040, 0.0695, 0.1246, 0.1215, 0.0794,\n",
            "        0.1374, 0.0962, 0.0936, 0.1067, 0.1226, 0.1350, 0.1340, 0.1268, 0.0712,\n",
            "        0.1291, 0.2262, 0.1214, 0.1354, 0.1373, 0.0767, 0.0925, 0.0982, 0.0736,\n",
            "        0.0883, 0.1043, 0.1634, 0.2313, 0.0681, 0.1005, 0.1021, 0.1016, 0.1505,\n",
            "        0.1073, 0.0918, 0.1070, 0.0819, 0.0702, 0.1145, 0.0864, 0.0788, 0.0725,\n",
            "        0.1494, 0.1390, 0.0933, 0.0522, 0.0954, 0.1457, 0.1316, 0.0769, 0.1500,\n",
            "        0.1428, 0.0722, 0.0698, 0.0862, 0.1052, 0.1430, 0.1328, 0.1988, 0.0815,\n",
            "        0.0682, 0.1406, 0.0625, 0.1082, 0.1250, 0.0752, 0.1434, 0.1479, 0.1070,\n",
            "        0.1550, 0.1625, 0.1037, 0.1222, 0.1293, 0.1351, 0.1018, 0.1060, 0.1003,\n",
            "        0.1008, 0.1310, 0.1077, 0.1012, 0.1510, 0.0596, 0.0985, 0.0621, 0.0618,\n",
            "        0.0996, 0.0866, 0.1036, 0.0825, 0.1431, 0.0655, 0.1066, 0.0831, 0.0878,\n",
            "        0.1126, 0.1043, 0.1090, 0.1833, 0.0975, 0.0979, 0.0912, 0.0988, 0.1231,\n",
            "        0.1059, 0.1607, 0.0511, 0.0811, 0.1116, 0.1085, 0.0916, 0.1425, 0.1382,\n",
            "        0.1648, 0.1006, 0.1133, 0.1276, 0.1288, 0.0909, 0.0750, 0.0869, 0.0757,\n",
            "        0.0628, 0.0984, 0.0694, 0.1159, 0.1555, 0.0955, 0.1112, 0.1704, 0.1587,\n",
            "        0.1524, 0.1112, 0.0417, 0.0922, 0.1210, 0.0827, 0.1371, 0.0973, 0.1096,\n",
            "        0.1420, 0.1717, 0.0878, 0.1679, 0.0601, 0.1255, 0.1665, 0.1123, 0.1470,\n",
            "        0.0576, 0.1283, 0.1804, 0.1002, 0.1378, 0.1142, 0.1055, 0.0812, 0.0845,\n",
            "        0.0969, 0.0651, 0.0998, 0.1252, 0.1047, 0.0855, 0.1738, 0.1461, 0.1233,\n",
            "        0.0656, 0.1042, 0.1872, 0.1090, 0.1707, 0.1156, 0.0465, 0.1497, 0.0739,\n",
            "        0.0979, 0.1052, 0.1189, 0.1540, 0.1349, 0.1028, 0.1048, 0.1963, 0.1099,\n",
            "        0.1324, 0.0543, 0.1058, 0.1015, 0.1050, 0.1265, 0.0467, 0.0965, 0.0617,\n",
            "        0.0920, 0.0831, 0.1799, 0.1204, 0.1026, 0.0822, 0.2190, 0.1938, 0.1522,\n",
            "        0.1003, 0.1607, 0.1257, 0.0985, 0.0542, 0.0830, 0.1288, 0.2287, 0.1200,\n",
            "        0.1932, 0.1052, 0.0909, 0.1130, 0.0825, 0.0710, 0.1291, 0.0979, 0.1224,\n",
            "        0.1099, 0.0891, 0.1211, 0.0812, 0.0817, 0.0979, 0.0922, 0.1369, 0.1025,\n",
            "        0.0790, 0.1075, 0.0811, 0.1393, 0.1209, 0.0849, 0.1234, 0.1105, 0.1476,\n",
            "        0.0954, 0.1323, 0.1069, 0.0812, 0.0721, 0.0855, 0.0856, 0.1287, 0.1007,\n",
            "        0.0732, 0.1026, 0.1024, 0.2997, 0.0914, 0.0859, 0.1832, 0.1187, 0.1244,\n",
            "        0.1184, 0.1012, 0.0723, 0.1163, 0.0825, 0.1351, 0.1059, 0.1248, 0.1394,\n",
            "        0.1018, 0.1215, 0.1245, 0.2928, 0.0863, 0.0963, 0.1693, 0.0970, 0.1313,\n",
            "        0.1103, 0.0977, 0.0970, 0.1175, 0.0802, 0.1690, 0.1732, 0.1190, 0.1351,\n",
            "        0.1126, 0.0926, 0.1290, 0.0647, 0.1023, 0.1481, 0.1086, 0.0831, 0.0759,\n",
            "        0.2942, 0.0872, 0.1114, 0.0579, 0.1407, 0.1285, 0.1421, 0.1410, 0.1168,\n",
            "        0.0824, 0.1487, 0.1013, 0.0898, 0.1334, 0.0772, 0.1246, 0.0831, 0.1198,\n",
            "        0.1453, 0.1211, 0.1166, 0.1863, 0.0517, 0.1824, 0.1111, 0.1079, 0.1013,\n",
            "        0.1739, 0.0737, 0.1265, 0.1227, 0.1305, 0.0997, 0.0793, 0.2833, 0.1293,\n",
            "        0.1229, 0.0811, 0.1165, 0.1066, 0.0932, 0.0990, 0.1102, 0.0482, 0.0867,\n",
            "        0.1073, 0.0632, 0.1117, 0.0883, 0.0914, 0.1157, 0.1387, 0.0634, 0.1124,\n",
            "        0.0544, 0.1149, 0.1656, 0.0849, 0.1279, 0.0909, 0.1329, 0.0745],\n",
            "       device='cuda:0')\n",
            "rank [408 450 426 484  98 183 367 172 130 357 137  73 153 214 340 118  95 358\n",
            " 369  71 326 471 255 411 473 308 353  75 477 321   2 438 298  97 328 286\n",
            " 429 437  56 138 300 103 127 303 506 270 182  43 226 123 262 361 287 283\n",
            " 225 105 336  41 288 359 238 188 206  99 331 198  91 460 446 223 395 305\n",
            "   1 322 114 203 468 156  70 222 247 212 207  32 268 456 297  29 125 457\n",
            " 454  74 217 154  50 422 390 199 501 269 132  31 310  27 162 175 294 385\n",
            " 104 174 230 440 419 167 337  46 168 463 124 510 213  40 342 397  90  15\n",
            " 204 431 235 481  83 485 229 171 375 443  94 366 274 403 455 307 508 273\n",
            "  37  34 169 479 347  57 362  64 302  18 318 220 421 465 159 425 413  65\n",
            " 393 323 260 116 486 113 480 166 377 228 160 424 173 380 469 292 391  89\n",
            " 354 368 467 134   3 439  92 335 412 414  76 435 458 155 470 488 417 282\n",
            " 500 329  47 129 505  19 194 311   7 151 272 372  93  22 441 252 503 304\n",
            " 136  13 497 265 452 289 285 474 146 394 121 432 492 341 378  85 296 254\n",
            "  66 327  54 447 266 219  55 475  28 236  48 388 189 495 224 191 398  69\n",
            " 165 249 489 232 261 420 100 344 312 370 211 334 126  63 346 102 339 149\n",
            " 319 101 181 253  30 325 120 157 227 245  60 338 355 406  62 386 407 445\n",
            " 186   8  38 423 231 187 345 461 476 237 415  61 234 404  36 271 185 233\n",
            " 360 309  81 317 482 243 491 259 363  39 240 280  35  26 178 122 257 333\n",
            " 376 383  59 433 256 295 430 434 315 349 140 148 428 163 108   5 284 202\n",
            " 396 133  42   0 164 128 200 490  72  87  79 442 177  49 291 384 351 190\n",
            "  52 267 107  96 499 409 141  53 258 371 275 509 110 462  16  23  67  82\n",
            "  17 379  14   9 180 498  84 142 251 131 299   4 451  77 277 494 244 195\n",
            " 427 210 410 402 320 401  33 507 392 314 112  78 448 250 352 466 365 293\n",
            " 150 246 418 373 459 356 192  51 382 215 381 399 313 264 389 487  88   6\n",
            " 436  10 161 483 109 147 387 196  45 111  80 135  21 464  12  25 205  86\n",
            " 176  24 449  20 278 221 276  68 511 332 478 179 405 197 416 208  44 400\n",
            " 170 374 193 117 209 145 158 139 281 152 216 184 106 324 248 143 316 144\n",
            " 444 502 496 279 218 241 242 350 301 239 119 453 306  11 504 343 364 115\n",
            " 201 472 263  58 493 348 330 290]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating the model after pruning, without finetuning:\")\n",
        "_, accuracy_model_prune, _ = validate(val_loader, model_prune, criterion)\n",
        "print(f\"This model's accuracy is {accuracy_model_prune}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYYQgJSTod75",
        "outputId": "4fe8e2b9-f8b0-4a8d-84fd-8e06f9410e1c"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model after pruning, without finetuning:\n",
            " * Acc@1 10.000 Acc@5 50.000\n",
            "This model's accuracy is 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune(model_prune, train_loader, val_loader, epochs=1, criterion=criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhr2zLsXoKDy",
        "outputId": "a7bc15f8-fc99-455e-fc90-4733ebb2e1ff"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 10.000 Acc@5 50.000\n",
            "learning_rate: 0.0001\n",
            "Epoch[0](0/391): Loss 2.2157 Prec@1(1,5) 21.09, 54.69 Lr 0.0001\n",
            "Epoch[0](39/391): Loss 1.9014 Prec@1(1,5) 42.48, 81.00 Lr 0.0001\n",
            "Epoch[0](78/391): Loss 1.6551 Prec@1(1,5) 53.70, 87.38 Lr 0.0001\n",
            "Epoch[0](117/391): Loss 1.4829 Prec@1(1,5) 59.31, 90.37 Lr 0.0001\n",
            "Epoch[0](156/391): Loss 1.3652 Prec@1(1,5) 62.75, 92.00 Lr 0.0001\n",
            "Epoch[0](195/391): Loss 1.2689 Prec@1(1,5) 65.39, 93.12 Lr 0.0001\n",
            "Epoch[0](234/391): Loss 1.1918 Prec@1(1,5) 67.44, 93.88 Lr 0.0001\n",
            "Epoch[0](273/391): Loss 1.1291 Prec@1(1,5) 69.06, 94.47 Lr 0.0001\n",
            "Epoch[0](312/391): Loss 1.0758 Prec@1(1,5) 70.39, 94.92 Lr 0.0001\n",
            "Epoch[0](351/391): Loss 1.0293 Prec@1(1,5) 71.57, 95.33 Lr 0.0001\n",
            "Epoch[0](390/391): Loss 0.9900 Prec@1(1,5) 72.56, 95.64 Lr 0.0001\n",
            " * Acc@1 79.470 Acc@5 98.350\n",
            "=>Best accuracy 79.470\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu3): ReLU(inplace=True)\n",
              "    (conv4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu4): ReLU(inplace=True)\n",
              "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu6): ReLU(inplace=True)\n",
              "    (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu7): ReLU(inplace=True)\n",
              "    (conv8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu8): ReLU(inplace=True)\n",
              "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu10): ReLU(inplace=True)\n",
              "    (conv11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu11): ReLU(inplace=True)\n",
              "    (conv12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu12): ReLU(inplace=True)\n",
              "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv14): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm14): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu14): ReLU(inplace=True)\n",
              "    (conv15): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu15): ReLU(inplace=True)\n",
              "    (conv16): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu16): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
              "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 4.3: Similarity**\n"
      ],
      "metadata": {
        "id": "42pQB-OgvXSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_similarity(model, model_ori):\n",
        "    oristate_dict = model_ori.state_dict()\n",
        "    state_dict = model.state_dict()\n",
        "    last_select_index = None  # Conv index selected in the previous layer\n",
        "\n",
        "    cnt = 0\n",
        "    for name, module in model.named_modules():\n",
        "        name = name.replace('module.', '')\n",
        "\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            cnt += 1\n",
        "            oriweight = oristate_dict[name + '.weight']\n",
        "            curweight = state_dict[name + '.weight']\n",
        "            orifilter_num = oriweight.size(0)\n",
        "            currentfilter_num = curweight.size(0)\n",
        "            print(f\"Processing layer {cnt}, original layer has {orifilter_num} filters, pruning model has {currentfilter_num} filters\")\n",
        "\n",
        "            if orifilter_num != currentfilter_num:\n",
        "                cov_id = cnt\n",
        "                #************ rank the filter's importance here\n",
        "                # print(oristate_dict[name + '.weight'].shape)\n",
        "                weight = oristate_dict[name + '.weight'].data\n",
        "                similarity_matrix = np.zeros((orifilter_num, orifilter_num))\n",
        "                for i in range(orifilter_num):\n",
        "                  for j in range(orifilter_num):\n",
        "                    # print(f'Computing the distance between filter {i} and filter {j}:')\n",
        "                    dist = torch.dist(weight[i], weight[j])\n",
        "                    similarity_matrix[i, j] = dist\n",
        "                    # print(dist)\n",
        "\n",
        "                print(similarity_matrix)\n",
        "                row_sums = np.sum(similarity_matrix, axis=1) # compute the sum of the distance of 1 filter to all other filters\n",
        "                rank = row_sums\n",
        "                #********************\n",
        "                # print(f\"rank {rank}\")\n",
        "                select_index = np.argsort(\n",
        "                    rank)[orifilter_num-currentfilter_num:]  # preserved filter id\n",
        "                select_index.sort()\n",
        "\n",
        "                if last_select_index is not None:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        for index_j, j in enumerate(last_select_index):\n",
        "                            state_dict[name + '.weight'][index_i][index_j] = \\\n",
        "                                oristate_dict[name + '.weight'][i][j]\n",
        "                else:\n",
        "                    for index_i, i in enumerate(select_index):\n",
        "                        state_dict[name + '.weight'][index_i] = \\\n",
        "                            oristate_dict[name + '.weight'][i]\n",
        "\n",
        "                last_select_index = select_index\n",
        "\n",
        "            elif last_select_index is not None:\n",
        "                for i in range(orifilter_num):\n",
        "                    for index_j, j in enumerate(last_select_index):\n",
        "                        state_dict[name + '.weight'][i][index_j] = \\\n",
        "                            oristate_dict[name + '.weight'][i][j]\n",
        "            else:\n",
        "                state_dict[name + '.weight'] = oriweight\n",
        "                last_select_index = None\n",
        "\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "CJ8Od02vpSKS"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_similarity(model_prune, model_ori)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFx3scSwp386",
        "outputId": "2e13b782-8e06-4e59-e3cf-bf0d6ee90236"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing layer 1, original layer has 64 filters, pruning model has 48 filters\n",
            "[[0.         1.74139035 1.59087288 ... 1.72806299 1.57414818 1.61237991]\n",
            " [1.74139035 0.         1.0449996  ... 1.0066036  0.88404632 0.85878301]\n",
            " [1.59087288 1.0449996  0.         ... 0.66931009 0.51807326 0.4106819 ]\n",
            " ...\n",
            " [1.72806299 1.0066036  0.66931009 ... 0.         0.57510459 0.59303981]\n",
            " [1.57414818 0.88404632 0.51807326 ... 0.57510459 0.         0.43026134]\n",
            " [1.61237991 0.85878301 0.4106819  ... 0.59303981 0.43026134 0.        ]]\n",
            "Processing layer 2, original layer has 64 filters, pruning model has 48 filters\n",
            "[[0.         1.18950224 1.54022479 ... 1.15816128 1.24496245 1.15282464]\n",
            " [1.18950224 0.         1.11939645 ... 1.11556244 0.97027659 1.26536036]\n",
            " [1.54022479 1.11939645 0.         ... 1.34018302 1.27003932 1.45868313]\n",
            " ...\n",
            " [1.15816128 1.11556244 1.34018302 ... 0.         1.18896043 1.27326846]\n",
            " [1.24496245 0.97027659 1.27003932 ... 1.18896043 0.         1.27268112]\n",
            " [1.15282464 1.26536036 1.45868313 ... 1.27326846 1.27268112 0.        ]]\n",
            "Processing layer 3, original layer has 128 filters, pruning model has 96 filters\n",
            "[[0.         1.42122185 1.22746193 ... 1.27757788 1.25067925 1.32484221]\n",
            " [1.42122185 0.         1.49760747 ... 1.40180743 1.3444494  1.55740595]\n",
            " [1.22746193 1.49760747 0.         ... 1.14871061 1.15785098 1.2968961 ]\n",
            " ...\n",
            " [1.27757788 1.40180743 1.14871061 ... 0.         1.02785122 1.15371168]\n",
            " [1.25067925 1.3444494  1.15785098 ... 1.02785122 0.         1.14477229]\n",
            " [1.32484221 1.55740595 1.2968961  ... 1.15371168 1.14477229 0.        ]]\n",
            "Processing layer 4, original layer has 128 filters, pruning model has 96 filters\n",
            "[[0.         1.46633482 1.60315692 ... 1.24838531 1.58495939 1.38703716]\n",
            " [1.46633482 0.         1.58788896 ... 1.38457549 1.64786541 1.40805209]\n",
            " [1.60315692 1.58788896 0.         ... 1.45373452 1.7336154  1.57224369]\n",
            " ...\n",
            " [1.24838531 1.38457549 1.45373452 ... 0.         1.55818081 1.32155156]\n",
            " [1.58495939 1.64786541 1.7336154  ... 1.55818081 0.         1.58488214]\n",
            " [1.38703716 1.40805209 1.57224369 ... 1.32155156 1.58488214 0.        ]]\n",
            "Processing layer 5, original layer has 256 filters, pruning model has 192 filters\n",
            "[[0.         1.29846847 1.33431685 ... 1.22870421 1.09841478 1.34573019]\n",
            " [1.29846847 0.         1.39555585 ... 1.29792893 1.18036664 1.33800352]\n",
            " [1.33431685 1.39555585 0.         ... 1.39792633 1.19068551 1.36408675]\n",
            " ...\n",
            " [1.22870421 1.29792893 1.39792633 ... 0.         1.13475263 1.32387829]\n",
            " [1.09841478 1.18036664 1.19068551 ... 1.13475263 0.         1.16552055]\n",
            " [1.34573019 1.33800352 1.36408675 ... 1.32387829 1.16552055 0.        ]]\n",
            "Processing layer 6, original layer has 256 filters, pruning model has 192 filters\n",
            "[[0.         1.49483824 1.47098637 ... 1.41265988 1.34934354 1.46011519]\n",
            " [1.49483824 0.         1.3395977  ... 1.23842406 1.24481761 1.27968168]\n",
            " [1.47098637 1.3395977  0.         ... 1.26577151 1.2824136  1.35073209]\n",
            " ...\n",
            " [1.41265988 1.23842406 1.26577151 ... 0.         1.18796015 1.24796057]\n",
            " [1.34934354 1.24481761 1.2824136  ... 1.18796015 0.         1.26724339]\n",
            " [1.46011519 1.27968168 1.35073209 ... 1.24796057 1.26724339 0.        ]]\n",
            "Processing layer 7, original layer has 256 filters, pruning model has 192 filters\n",
            "[[0.         1.11892903 1.15290225 ... 1.22711694 1.13740611 1.08098245]\n",
            " [1.11892903 0.         1.21141601 ... 1.24150646 1.23232353 1.17370617]\n",
            " [1.15290225 1.21141601 0.         ... 1.24543631 1.20642948 1.13828218]\n",
            " ...\n",
            " [1.22711694 1.24150646 1.24543631 ... 0.         1.29025269 1.21557927]\n",
            " [1.13740611 1.23232353 1.20642948 ... 1.29025269 0.         1.1358875 ]\n",
            " [1.08098245 1.17370617 1.13828218 ... 1.21557927 1.1358875  0.        ]]\n",
            "Processing layer 8, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.74783897 0.53641456 ... 0.65880537 0.69070882 0.62650144]\n",
            " [0.74783897 0.         0.70040119 ... 0.8114686  0.83108282 0.81882209]\n",
            " [0.53641456 0.70040119 0.         ... 0.64930391 0.66257775 0.65391576]\n",
            " ...\n",
            " [0.65880537 0.8114686  0.64930391 ... 0.         0.74297321 0.72759873]\n",
            " [0.69070882 0.83108282 0.66257775 ... 0.74297321 0.         0.77549708]\n",
            " [0.62650144 0.81882209 0.65391576 ... 0.72759873 0.77549708 0.        ]]\n",
            "Processing layer 9, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.44937032 0.44717467 ... 0.44214386 0.42544821 0.39422199]\n",
            " [0.44937032 0.         0.43297523 ... 0.45674026 0.46420765 0.44007283]\n",
            " [0.44717467 0.43297523 0.         ... 0.34375292 0.3272014  0.3267366 ]\n",
            " ...\n",
            " [0.44214386 0.45674026 0.34375292 ... 0.         0.28739169 0.33429554]\n",
            " [0.42544821 0.46420765 0.3272014  ... 0.28739169 0.         0.35326961]\n",
            " [0.39422199 0.44007283 0.3267366  ... 0.33429554 0.35326961 0.        ]]\n",
            "Processing layer 10, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.24060258 0.24136184 ... 0.23888567 0.34078321 0.31026241]\n",
            " [0.24060258 0.         0.20055246 ... 0.23511079 0.30632436 0.26569974]\n",
            " [0.24136184 0.20055246 0.         ... 0.20995007 0.27098751 0.2573823 ]\n",
            " ...\n",
            " [0.23888567 0.23511079 0.20995007 ... 0.         0.30488899 0.26040235]\n",
            " [0.34078321 0.30632436 0.27098751 ... 0.30488899 0.         0.31724426]\n",
            " [0.31026241 0.26569974 0.2573823  ... 0.26040235 0.31724426 0.        ]]\n",
            "Processing layer 11, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.25313532 0.15862787 ... 0.18030564 0.16444081 0.12301059]\n",
            " [0.25313532 0.         0.23972732 ... 0.29272699 0.26680851 0.23072371]\n",
            " [0.15862787 0.23972732 0.         ... 0.17284445 0.13971126 0.11121501]\n",
            " ...\n",
            " [0.18030564 0.29272699 0.17284445 ... 0.         0.1739077  0.1441773 ]\n",
            " [0.16444081 0.26680851 0.13971126 ... 0.1739077  0.         0.12007354]\n",
            " [0.12301059 0.23072371 0.11121501 ... 0.1441773  0.12007354 0.        ]]\n",
            "Processing layer 12, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.15282945 0.1384365  ... 0.17996098 0.16928835 0.12805164]\n",
            " [0.15282945 0.         0.09452864 ... 0.1907845  0.17421071 0.15122023]\n",
            " [0.1384365  0.09452864 0.         ... 0.15639731 0.16523457 0.10234247]\n",
            " ...\n",
            " [0.17996098 0.1907845  0.15639731 ... 0.         0.17768517 0.1722673 ]\n",
            " [0.16928835 0.17421071 0.16523457 ... 0.17768517 0.         0.1745384 ]\n",
            " [0.12805164 0.15122023 0.10234247 ... 0.1722673  0.1745384  0.        ]]\n",
            "Processing layer 13, original layer has 512 filters, pruning model has 384 filters\n",
            "[[0.         0.17427406 0.19477567 ... 0.11588519 0.18578708 0.12594552]\n",
            " [0.17427406 0.         0.23022732 ... 0.13994159 0.21605481 0.17047261]\n",
            " [0.19477567 0.23022732 0.         ... 0.19367294 0.23759399 0.15547395]\n",
            " ...\n",
            " [0.11588519 0.13994159 0.19367294 ... 0.         0.16849667 0.10820013]\n",
            " [0.18578708 0.21605481 0.23759399 ... 0.16849667 0.         0.14886674]\n",
            " [0.12594552 0.17047261 0.15547395 ... 0.10820013 0.14886674 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating the model after pruning, without finetuning:\")\n",
        "_, accuracy_model_prune, _ = validate(val_loader, model_prune, criterion)\n",
        "print(f\"This model's accuracy is {accuracy_model_prune}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Bn9qQAu9gv",
        "outputId": "75f6b33a-3a72-4a1d-8514-7ab7abd2f0bc"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model after pruning, without finetuning:\n",
            " * Acc@1 10.000 Acc@5 50.020\n",
            "This model's accuracy is 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune(model_prune, train_loader, val_loader, epochs=1, criterion=criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyLGmopFvAld",
        "outputId": "f6575d71-8081-4520-ce20-ad4efd405e31"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Acc@1 10.000 Acc@5 50.020\n",
            "learning_rate: 0.0001\n",
            "Epoch[0](0/391): Loss 2.1867 Prec@1(1,5) 20.31, 67.19 Lr 0.0001\n",
            "Epoch[0](39/391): Loss 1.9700 Prec@1(1,5) 33.50, 78.67 Lr 0.0001\n",
            "Epoch[0](78/391): Loss 1.7097 Prec@1(1,5) 49.00, 85.36 Lr 0.0001\n",
            "Epoch[0](117/391): Loss 1.5341 Prec@1(1,5) 55.89, 88.79 Lr 0.0001\n",
            "Epoch[0](156/391): Loss 1.4014 Prec@1(1,5) 60.56, 90.80 Lr 0.0001\n",
            "Epoch[0](195/391): Loss 1.2996 Prec@1(1,5) 63.79, 92.12 Lr 0.0001\n",
            "Epoch[0](234/391): Loss 1.2187 Prec@1(1,5) 66.18, 93.06 Lr 0.0001\n",
            "Epoch[0](273/391): Loss 1.1466 Prec@1(1,5) 68.31, 93.80 Lr 0.0001\n",
            "Epoch[0](312/391): Loss 1.0864 Prec@1(1,5) 69.98, 94.41 Lr 0.0001\n",
            "Epoch[0](351/391): Loss 1.0340 Prec@1(1,5) 71.46, 94.85 Lr 0.0001\n",
            "Epoch[0](390/391): Loss 0.9885 Prec@1(1,5) 72.69, 95.26 Lr 0.0001\n",
            " * Acc@1 80.640 Acc@5 98.350\n",
            "=>Best accuracy 80.640\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu3): ReLU(inplace=True)\n",
              "    (conv4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu4): ReLU(inplace=True)\n",
              "    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv6): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu6): ReLU(inplace=True)\n",
              "    (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu7): ReLU(inplace=True)\n",
              "    (conv8): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu8): ReLU(inplace=True)\n",
              "    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm10): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu10): ReLU(inplace=True)\n",
              "    (conv11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu11): ReLU(inplace=True)\n",
              "    (conv12): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu12): ReLU(inplace=True)\n",
              "    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv14): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm14): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu14): ReLU(inplace=True)\n",
              "    (conv15): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm15): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu15): ReLU(inplace=True)\n",
              "    (conv16): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (norm16): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu16): ReLU(inplace=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (linear1): Linear(in_features=384, out_features=512, bias=True)\n",
              "    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 4: Analysis & Conclusion**\n"
      ],
      "metadata": {
        "id": "Mj3E7RhifLd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some questions for you to consider while working on this project:\n",
        "1. **What is the relation between compression rate, reduced complexity/parameters, and accuracy?**\n",
        "   - How does increasing the compression rate affect the model's accuracy?\n",
        "   - Can you explain the trade-off between model complexity (number of parameters) and accuracy?\n",
        "\n",
        "2. **How do you decide the importance of filters?**\n",
        "   - What criteria can be used to determine the importance of filters in a convolutional neural network?\n",
        "   - How do similarity-based filter pruning techniques identify redundant or less important filters?\n",
        "   - Can you explain the concept of filter importance in the context of model efficiency and effectiveness?\n",
        "\n",
        "3. **What are the implications of pruning on model performance and inference speed?**\n",
        "   - How does pruning affect the inference speed of a model?\n",
        "   - Can you discuss the impact of pruning on model accuracy during inference?\n",
        "   - What strategies can be employed to mitigate any potential loss of accuracy after pruning?\n",
        "\n",
        "4. **How does fine-tuning help improve the performance of pruned models?**\n",
        "   - What is the purpose of fine-tuning a pruned model?\n",
        "   - How does fine-tuning help the model adapt to the changes introduced by pruning?\n",
        "   - Can you explain any challenges or considerations when fine-tuning pruned models?\n",
        "\n",
        "5. **What are some alternative pruning techniques, and how do they compare to similarity-based pruning?**\n",
        "   - Can you describe magnitude-based pruning and its advantages/disadvantages compared to similarity-based pruning?\n",
        "   - What is sensitivity-based pruning, and how does it differ from similarity-based pruning?\n",
        "   - Are there any hybrid approaches that combine multiple pruning techniques for better results?"
      ],
      "metadata": {
        "id": "zj54KZqqfgME"
      }
    }
  ]
}